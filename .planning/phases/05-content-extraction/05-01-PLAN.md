---
phase: 05-content-extraction
plan: 01
depends_on: []
files_modified:
  - src/tools/extract-page.ts
  - package.json
---

# Plan 05-01: Single-Page Extraction Tool

## Objective

Build `src/tools/extract-page.ts` — a CLI tool that fetches a single German servicePage from Sanity by `_id` and outputs clean, structured JSON ready for downstream processing. This is the foundational extraction module that 05-02 (batch) and 05-03 (normalization) will build on.

## Execution Context

```yaml
strategy: A  # Fully autonomous, no checkpoints
estimated_tasks: 2
estimated_minutes: 20
parallel_safe: true  # No shared state with other plans
```

## Context

### What We Know

- Full servicePage schema documented in 01-04-SUMMARY.md (14 fields: title, slug, heroSection, overview, approach, services, faq, seo, testimonial, parent, references, language, _id, _type)
- 64 existing source JSON files (e.g., `kyc-source.json`) show exact format — they are raw Sanity documents with ALL fields
- No extraction scripts exist in repo (Decision #2) — building from scratch
- Shared infrastructure available: `sanity-client.ts`, `errors.ts` (withRetry, SanityApiError), `logger.ts`
- `translation-queue.json` contains 587 entries with `germanId` field — the IDs we'll extract

### System Fields to Strip

These fields appear on Sanity documents but should NOT be in extraction output:
- `_createdAt`, `_updatedAt`, `_rev` (system timestamps/revision)
- `__i18n_lang`, `__i18n_refs`, `__i18n_base` (legacy i18n — unreliable, Decision #1)
- `_system` (Sanity Studio tracking — present on 162 docs)

### Fields to Preserve

**Translatable content fields** (all text will be translated in Phase 6):
- `title` — page title
- `heroSection` — { heading, tagline, description, benefits[], heroImage }
- `overview` — { heading, description, additionalInfo, alert, points[], serviceDescription, servicePoints[], whyUs }
- `approach` — { title, description, points[] }
- `services[]` — each { title, description, features[] }
- `faq[]` — each { question, answer }
- `seo` — { title, description, keywords }
- `testimonial` — { name, position, company, quote }

**Structural metadata** (needed for English doc creation, NOT translated):
- `_id` — German document ID (used to derive English ID convention: `<id>-en`)
- `_type` — always `"servicePage"`
- `language` — always `"de"` in extraction (Phase 8 will set to `"en"`)
- `slug` — full object `{ _type: "slug", current: "..." }` (Phase 8 will translate path segments + add `-en`)
- `parent` — `{ _ref: "...", _type: "reference" }` (Phase 9 will adjust for English parent)
- `references` — `{ topLevelParent: { _ref: "...", _type: "reference" } }` (Phase 9 will adjust)

**Array item keys** (preserve `_key` and `_type` on each array item — Sanity requires these):
- `heroSection.benefits[]._key`, `._type`
- `overview.points[]._key`, `overview.servicePoints[]._key`, `overview.whyUs.points[]._key`
- `approach.points[]._key`
- `services[]._key`, `services[].features[]._key`
- `faq[]._key`

### GROQ Query Design

Fetch the complete document and strip in code (simpler than projecting every nested field):
```groq
*[_type == "servicePage" && language == "de" && _id == $id][0]
```

Then strip system fields in TypeScript. This is simpler and more maintainable than a massive GROQ projection, and matches the pattern in `detect-untranslated.ts`.

### Established Patterns to Follow

From existing tools in `src/tools/`:
- Import from `../lib/sanity-client.js`, `../lib/errors.js`, `../lib/logger.js`
- Use `withRetry()` for all Sanity API calls
- Use `logger.*` methods with emoji conventions
- Add `import.meta.url` guard for CLI entry point
- Export core function and types for module consumption
- Parse CLI args manually (no external arg parser)
- Write JSON output with `JSON.stringify(data, null, 2)`

## Tasks

### Task 1: Create extraction tool

Create `src/tools/extract-page.ts` with:

1. **Types** (all exported):
   - `ExtractedPage` — the clean extraction output interface matching servicePage schema
   - Include all fields from "Fields to Preserve" above

2. **Core function** `extractPage(germanId: string): Promise<ExtractedPage>`:
   - Fetch full document via GROQ: `*[_type == "servicePage" && language == "de" && _id == $id][0]`
   - Wrap in `withRetry()` with standard options `{ maxRetries: 3, delayMs: 1000, backoffMultiplier: 2 }`
   - If document not found (null), throw `ValidationError` with context `{ germanId }`
   - If `language !== "de"`, throw `ValidationError` (safety check)
   - Strip system fields: delete `_createdAt`, `_updatedAt`, `_rev`, `__i18n_lang`, `__i18n_refs`, `__i18n_base`, `_system`
   - Return typed `ExtractedPage`

3. **CLI interface**:
   - `--id <germanId>` — required, the Sanity document ID to extract
   - `--output <path>` — optional, write JSON to file (default: stdout)
   - `--quiet` — suppress logger output (useful when piping stdout)
   - Display extracted page summary (title, slug, FAQ count, services count)
   - Add `import.meta.url` guard so main() only runs when executed directly

4. **Add npm script** to `package.json`:
   - `"extract": "tsx src/tools/extract-page.ts"`

### Task 2: Verify with live Sanity data

Run the tool against 3 sample pages from the translation queue to verify:

1. **Root-level page** (depth 0): Use first entry from queue (e.g., `ai-governance`)
   - Verify: all fields present, no system fields, slug is correct

2. **Deep-nested page** (depth 3+): Pick a page with depth >= 3
   - Verify: parent and references.topLevelParent present, slug contains hierarchy path

3. **Comparison with existing source JSON**: If one of the 64 existing source JSONs matches a queued page, extract it and compare field structure

For each verification:
- Run `npx tsx src/tools/extract-page.ts --id <id> --output test-extract.json`
- Check output has all expected fields
- Check output has NO system fields
- Check FAQ count and structure match expectations
- Delete test output files after verification

## Verification

- [ ] `npx tsx src/tools/extract-page.ts --id <any-german-id>` produces valid JSON
- [ ] Output contains all 14 expected field groups (title, slug, heroSection, overview, approach, services, faq, seo, testimonial, parent, references, language, _id, _type)
- [ ] Output does NOT contain: _createdAt, _updatedAt, _rev, __i18n_lang, __i18n_refs, __i18n_base, _system
- [ ] Array items preserve _key and _type fields
- [ ] `extractPage()` function and `ExtractedPage` type are properly exported
- [ ] `npm run extract -- --id kyc` works via npm script
- [ ] Non-existent ID throws ValidationError with helpful message

## Success Criteria

A working single-page extraction tool that:
1. Fetches any German servicePage by ID from Sanity
2. Strips all system/legacy fields
3. Outputs clean JSON matching the known schema
4. Exports core function + types for use by 05-02 and 05-03
5. Follows established CLI patterns from Phase 3-4 tools

## Output

- `src/tools/extract-page.ts` — extraction tool (~120-180 lines)
- Updated `package.json` with `"extract"` script
