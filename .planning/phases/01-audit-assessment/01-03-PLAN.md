---
phase: 01-audit-assessment
plan: 03
type: execute
depends_on: []
files_modified: [.planning/phases/01-audit-assessment/01-03-SUMMARY.md]
---

<objective>
Audit completed translations â€” verify quality against the 11 rules and check Sanity upload status.

Purpose: Understand the actual quality of existing translations and whether they're properly published in Sanity.
Output: Quality assessment of completed translations and Sanity publication status.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/ARCHITECTURE.md
@translation-batch/TRANSLATION-INSTRUCTIONS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Sample and verify completed translations against the 11 mandatory rules</name>
  <files>.planning/phases/01-audit-assessment/01-03-SUMMARY.md</files>
  <action>
  1. Select 5 completed translation files from `ADVISORI Sanity Migration/de-pages-created/`. Pick a diverse sample:
     - 1 from DORA domain
     - 1 from SIEM domain
     - 1 from CRR/CRD domain
     - 1 from IAM domain
     - 1 from Intelligent Automation domain
  2. For EACH sampled file, verify against all 11 rules:
     - Rule 1: Language field is 'en' (not 'de')
     - Rule 2: Content is translated German (not hallucinated)
     - Rule 3: No duplicate FAQs
     - Rule 4: Testimonial has all 5 fields (quote, name, position, company, author) if present
     - Rule 5: Formatting preserved (emojis, bullets, line breaks)
     - Rule 6: Optional fields only present if in German source
     - Rule 7: Unique _key for array items
     - Rule 8: Slug in kebab-case ending with '-en'
     - Rule 9: FAQ count matches (check against batch files)
     - Rule 10: All sections present
     - Rule 11: _type: "object" for array items
  3. Also check for each file:
     - Does it use hardcoded Sanity credentials?
     - Does it set __i18n_lang and __i18n_base?
     - What Sanity operation does it use? (createOrReplace, create, patch)
  4. Score each translation: pass/fail per rule, overall quality rating

  READ-ONLY. Do not modify translation files.
  </action>
  <verify>5 files sampled across different domains. All 11 rules checked for each. Score table produced.</verify>
  <done>Quality scorecard for 5 sampled translations with per-rule pass/fail and overall assessment</done>
</task>

<task type="auto">
  <name>Task 2: Query Sanity to verify sampled translations are published and accessible</name>
  <files>.planning/phases/01-audit-assessment/01-03-SUMMARY.md</files>
  <action>
  1. Use the Sanity credentials from the .env file or from the existing scripts to connect
  2. For each of the 5 sampled translations from Task 1, query Sanity:
     - Does the English document exist? (query by slug with -en suffix)
     - Is it published (not just a draft)?
     - Does it have __i18n_lang set to 'en'?
     - Does it have __i18n_base referencing the German document?
     - Does the German document have __i18n_refs pointing back to the English version?
  3. Also run a broader query:
     - How many English servicePage documents exist in Sanity total?
     - How many German servicePage documents exist?
     - How many German pages have linked English translations?
  4. Compare Sanity counts against the TRANSLATION-TRACKER.md counts (62 completed)

  Use a read-only GROQ query via npx tsx with @sanity/client. Do NOT create or modify any Sanity documents.
  Create a small temporary script to run the queries, then delete it after capturing results.
  </action>
  <verify>Sanity query returns results. At least 3 of 5 sampled pages found in Sanity. Aggregate counts captured.</verify>
  <done>Sanity publication status for sampled translations, plus aggregate counts compared to local tracker</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] 5 translations sampled from different domains
- [ ] All 11 rules checked for each sample
- [ ] Sanity queries executed successfully
- [ ] Publication status verified for samples
- [ ] Aggregate counts compared (local tracker vs Sanity)
- [ ] Quality scorecard produced
</verification>

<success_criteria>

- Clear picture of translation quality across existing work
- Sanity publication status known (published, draft, missing)
- Local tracker accuracy verified against Sanity reality
- Common quality issues identified (if any)
</success_criteria>

<output>
After completion, create `.planning/phases/01-audit-assessment/01-03-SUMMARY.md`
</output>
