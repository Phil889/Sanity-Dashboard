import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating Gefahren durch KI page with FAQs batch 3...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'gefahren-durch-ki' })
    
    if (!existingDoc) {
      throw new Error('Document "gefahren-durch-ki" not found')
    }
    
    // Create new FAQs
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 9),
        question: 'Welche Risiken entstehen durch KI-Halluzinationen und wie kann ADVISORI diese f√ºr kritische Gesch√§ftsentscheidungen minimieren?',
        answer: "KI-Halluzinationen - die Generierung falscher oder erfundener Informationen durch KI-Systeme - stellen eine der subtilsten und gleichzeitig gef√§hrlichsten Bedrohungen f√ºr Unternehmen dar, die KI f√ºr kritische Entscheidungen einsetzen. Diese Ph√§nomene k√∂nnen zu fehlerhaften Gesch√§ftsentscheidungen, rechtlichen Problemen und Reputationssch√§den f√ºhren. ADVISORI entwickelt umfassende Frameworks zur Erkennung, Bewertung und Minimierung von Halluzinationsrisiken in gesch√§ftskritischen KI-Anwendungen.\n\nüß† Halluzinations-Mechanismen und Gesch√§ftsrisiken:\n‚Ä¢ Konfabulation: KI-Systeme generieren plausibel klingende, aber faktisch falsche Informationen, die in Berichten oder Analysen verwendet werden k√∂nnten.\n‚Ä¢ Source Confusion: Vermischung oder falsche Zuordnung von Informationen aus verschiedenen Quellen, was zu irref√ºhrenden Schlussfolgerungen f√ºhrt.\n‚Ä¢ Overconfident Predictions: √úberm√§√üiges Vertrauen in unsichere Vorhersagen, die zu riskanten Gesch√§ftsentscheidungen f√ºhren k√∂nnen.\n‚Ä¢ Context Drift: Verlust des urspr√ºnglichen Kontexts bei l√§ngeren Interaktionen, was zu inkonsistenten oder widerspr√ºchlichen Aussagen f√ºhrt.\n\nüîç ADVISORI's Hallucination Detection Framework:\n‚Ä¢ Multi-Source Verification: Implementierung von Systemen, die KI-Ausgaben automatisch gegen multiple vertrauensw√ºrdige Quellen validieren.\n‚Ä¢ Confidence Scoring und Uncertainty Quantification: Entwicklung von Metriken zur Bewertung der Zuverl√§ssigkeit von KI-Ausgaben.\n‚Ä¢ Fact-Checking Pipelines: Integration automatisierter Fact-Checking-Systeme zur Verifikation kritischer Informationen.\n‚Ä¢ Human-in-the-Loop Validation: Etablierung von Prozessen f√ºr menschliche √úberpr√ºfung bei kritischen Entscheidungen.\n\nüõ°Ô∏è Proactive Mitigation Strategies:\n‚Ä¢ Retrieval-Augmented Generation: Implementierung von RAG-Systemen, die KI-Antworten auf vertrauensw√ºrdige Wissensdatenbanken st√ºtzen.\n‚Ä¢ Ensemble Methods: Verwendung mehrerer KI-Modelle zur Kreuzvalidierung und Konsensbildung.\n‚Ä¢ Structured Output Formats: Entwicklung strukturierter Ausgabeformate, die Quellenangaben und Konfidenzwerte enthalten.\n‚Ä¢ Domain-Specific Fine-Tuning: Anpassung von KI-Modellen an spezifische Gesch√§ftsbereiche zur Reduzierung von Halluzinationen.\n\nüìä Business Process Integration:\n‚Ä¢ Risk-Aware Decision Frameworks: Integration von Halluzinationsrisiken in Gesch√§ftsentscheidungsprozesse.\n‚Ä¢ Escalation Procedures: Etablierung klarer Eskalationswege bei Unsicherheiten oder widerspr√ºchlichen KI-Ausgaben.\n‚Ä¢ Audit Trails: Vollst√§ndige Dokumentation von KI-Entscheidungen f√ºr sp√§tere √úberpr√ºfung und Compliance.\n‚Ä¢ Continuous Learning: Implementierung von Feedback-Loops zur kontinuierlichen Verbesserung der Halluzinations-Erkennung.\n\nüéØ Quality Assurance und Monitoring:\n‚Ä¢ Real-time Monitoring: Kontinuierliche √úberwachung von KI-Ausgaben auf Anzeichen von Halluzinationen.\n‚Ä¢ Performance Metrics: Entwicklung spezifischer KPIs zur Messung der Faktentreue und Zuverl√§ssigkeit.\n‚Ä¢ Regular Model Evaluation: Systematische Bewertung der Halluzinationsneigung verschiedener KI-Modelle.\n‚Ä¢ Incident Response: Schnelle Reaktionsverfahren bei Entdeckung kritischer Halluzinationen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 10),
        question: 'Wie sch√ºtzt ADVISORI vor Prompt Injection Angriffen und welche Risiken entstehen durch manipulierte KI-Eingaben?',
        answer: "Prompt Injection Angriffe stellen eine neue Kategorie von Sicherheitsbedrohungen dar, die spezifisch f√ºr Large Language Models und generative KI-Systeme entwickelt wurden. Diese Angriffe nutzen die nat√ºrlichsprachliche Schnittstelle von KI-Systemen aus, um deren Verhalten zu manipulieren oder unbeabsichtigte Aktionen auszul√∂sen. ADVISORI entwickelt spezialisierte Abwehrstrategien gegen diese emerging threats, die sowohl technische als auch organisatorische Ma√ünahmen umfassen.\n\nüíâ Prompt Injection Angriffsvektoren:\n‚Ä¢ Direct Prompt Injection: Direkte Manipulation von Systemprompts durch b√∂sartige Benutzereingaben zur Umgehung von Sicherheitsrichtlinien.\n‚Ä¢ Indirect Prompt Injection: Einschleusung manipulativer Anweisungen √ºber externe Datenquellen wie Dokumente oder Webseiten.\n‚Ä¢ Jailbreaking: Umgehung von Sicherheitsbeschr√§nkungen durch clevere Formulierungen oder Rollenspiele.\n‚Ä¢ Data Exfiltration: Ausnutzung von Prompt Injection zur unbefugten Extraktion sensibler Informationen aus KI-Systemen.\n\nüõ°Ô∏è ADVISORI's Multi-Layer Defense Strategy:\n‚Ä¢ Input Sanitization und Validation: Implementierung robuster Filter zur Erkennung und Neutralisierung verd√§chtiger Eingaben.\n‚Ä¢ Prompt Isolation: Trennung von Systemprompts und Benutzereingaben durch technische Barrieren.\n‚Ä¢ Context Boundary Enforcement: Strikte Durchsetzung von Kontextgrenzen zur Verhinderung von Prompt Leakage.\n‚Ä¢ Output Filtering: √úberwachung und Filterung von KI-Ausgaben zur Verhinderung unbeabsichtigter Informationspreisgabe.\n\nüîç Advanced Detection Mechanisms:\n‚Ä¢ Behavioral Analysis: √úberwachung von KI-Systemverhalten zur Erkennung ungew√∂hnlicher oder verd√§chtiger Aktivit√§ten.\n‚Ä¢ Semantic Analysis: Tiefere Analyse der Bedeutung und Absicht von Benutzereingaben.\n‚Ä¢ Pattern Recognition: Identifikation bekannter Injection-Muster und Angriffssignaturen.\n‚Ä¢ Anomaly Detection: Erkennung von Abweichungen vom normalen Systemverhalten.\n\nüèóÔ∏è Secure Architecture Design:\n‚Ä¢ Principle of Least Privilege: Minimierung der Berechtigungen und F√§higkeiten von KI-Systemen.\n‚Ä¢ Sandboxing: Isolation von KI-Systemen in sicheren Umgebungen mit begrenzten Zugriffsm√∂glichkeiten.\n‚Ä¢ API Security: Robuste Sicherheitsma√ünahmen f√ºr KI-APIs und Schnittstellen.\n‚Ä¢ Access Controls: Granulare Zugriffskontrolle f√ºr verschiedene KI-Funktionen und Datenquellen.\n\nüìä Monitoring und Response:\n‚Ä¢ Real-time Threat Detection: Sofortige Erkennung und Reaktion auf Prompt Injection Versuche.\n‚Ä¢ Incident Response Procedures: Spezialisierte Verfahren f√ºr die Behandlung von Prompt Injection Incidents.\n‚Ä¢ Forensic Capabilities: Detaillierte Analyse und Nachverfolgung von Angriffsversuchen.\n‚Ä¢ Continuous Improvement: Regelm√§√üige Aktualisierung der Abwehrma√ünahmen basierend auf neuen Bedrohungen.\n\nüéì Training und Awareness:\n‚Ä¢ Security Training: Schulung von Entwicklern und Nutzern zu Prompt Injection Risiken.\n‚Ä¢ Best Practices: Entwicklung und Verbreitung von Sicherheits-Best-Practices f√ºr KI-Systeme.\n‚Ä¢ Red Team Exercises: Regelm√§√üige Penetrationstests zur Bewertung der Wirksamkeit von Schutzma√ünahmen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 11),
        question: 'Welche spezifischen Risiken entstehen durch KI-Deepfakes und wie implementiert ADVISORI Schutzma√ünahmen gegen synthetische Medien?',
        answer: "Deepfakes und synthetische Medien stellen eine wachsende Bedrohung f√ºr Unternehmen dar, da sie f√ºr Betrug, Manipulation und Reputationssch√§den eingesetzt werden k√∂nnen. Diese Technologien k√∂nnen t√§uschend echte Audio-, Video- und Bildmaterialien erstellen, die schwer von authentischen Inhalten zu unterscheiden sind. ADVISORI entwickelt umfassende Detection- und Prevention-Strategien zum Schutz vor den vielf√§ltigen Risiken synthetischer Medien.\n\nüé≠ Deepfake Bedrohungslandschaft:\n‚Ä¢ CEO Fraud und Voice Cloning: Nachahmung von F√ºhrungskr√§ften f√ºr Betrugsversuche oder unbefugte Anweisungen.\n‚Ä¢ Brand Impersonation: Erstellung gef√§lschter Inhalte zur Sch√§digung der Unternehmensreputation.\n‚Ä¢ Social Engineering: Nutzung synthetischer Medien f√ºr sophisticated Phishing und Manipulation.\n‚Ä¢ Market Manipulation: Verbreitung falscher Informationen zur Beeinflussung von Aktienkursen oder Gesch√§ftsentscheidungen.\n\nüîç ADVISORI's Deepfake Detection Framework:\n‚Ä¢ Multi-Modal Analysis: Kombination verschiedener Erkennungstechniken f√ºr Audio, Video und Bildmaterial.\n‚Ä¢ Temporal Inconsistency Detection: Analyse zeitlicher Inkonsistenzen in Videomaterial.\n‚Ä¢ Biometric Verification: √úberpr√ºfung biometrischer Merkmale zur Authentifizierung von Personen.\n‚Ä¢ Blockchain-based Provenance: Implementierung unver√§nderlicher Herkunftsnachweise f√ºr authentische Medien.\n\nüõ°Ô∏è Proactive Protection Measures:\n‚Ä¢ Media Authentication Systems: Entwicklung von Systemen zur Verifikation der Authentizit√§t von Medieninhalten.\n‚Ä¢ Digital Watermarking: Einbettung unsichtbarer Wasserzeichen in authentische Unternehmensinhalte.\n‚Ä¢ Voice Biometrics: Implementierung von Stimmerkennungssystemen f√ºr kritische Kommunikation.\n‚Ä¢ Content Verification Pipelines: Automatisierte √úberpr√ºfung eingehender Medieninhalte.\n\nüè¢ Organizational Safeguards:\n‚Ä¢ Verification Protocols: Etablierung strenger Verifikationsverfahren f√ºr kritische Kommunikation.\n‚Ä¢ Multi-Channel Confirmation: Best√§tigung wichtiger Anweisungen √ºber mehrere unabh√§ngige Kan√§le.\n‚Ä¢ Employee Training: Schulung der Mitarbeiter zur Erkennung von Deepfakes und synthetischen Medien.\n‚Ä¢ Incident Response Plans: Spezialisierte Verfahren f√ºr den Umgang mit Deepfake-Angriffen.\n\nüìä Monitoring und Intelligence:\n‚Ä¢ Dark Web Monitoring: √úberwachung von Plattformen auf potenzielle Deepfake-Bedrohungen gegen das Unternehmen.\n‚Ä¢ Brand Protection: Kontinuierliche √úberwachung des Internets auf gef√§lschte Unternehmensinhalte.\n‚Ä¢ Threat Intelligence: Integration aktueller Informationen √ºber neue Deepfake-Technologien und -Bedrohungen.\n‚Ä¢ Legal Preparedness: Vorbereitung rechtlicher Schritte gegen Deepfake-Missbrauch.\n\nüî¨ Technical Innovation:\n‚Ä¢ AI-powered Detection: Einsatz fortschrittlicher KI-Systeme zur Deepfake-Erkennung.\n‚Ä¢ Real-time Analysis: Entwicklung von Systemen f√ºr die Echtzeit-Analyse verd√§chtiger Inhalte.\n‚Ä¢ Cross-Platform Integration: Integration von Deepfake-Detection in verschiedene Kommunikationsplattformen.\n‚Ä¢ Continuous Learning: Anpassung der Erkennungssysteme an neue Deepfake-Technologien."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 12),
        question: 'Wie adressiert ADVISORI die Risiken von KI-Vendor Lock-in und gew√§hrleistet strategische Flexibilit√§t bei KI-Investitionen?',
        answer: "KI-Vendor Lock-in stellt ein erhebliches strategisches Risiko f√ºr Unternehmen dar, da es die Flexibilit√§t einschr√§nkt, Kosten erh√∂ht und die Abh√§ngigkeit von einzelnen Anbietern verst√§rkt. In der schnelllebigen KI-Landschaft kann Lock-in dazu f√ºhren, dass Unternehmen nicht von technologischen Fortschritten profitieren k√∂nnen oder bei Problemen mit dem Anbieter handlungsunf√§hig werden. ADVISORI entwickelt strategische Frameworks zur Vermeidung von Vendor Lock-in und zur Gew√§hrleistung langfristiger Flexibilit√§t.\n\nüîí Vendor Lock-in Risikokategorien:\n‚Ä¢ Technical Lock-in: Abh√§ngigkeit von propriet√§ren APIs, Datenformaten oder Infrastrukturen, die Migration erschweren.\n‚Ä¢ Data Lock-in: Schwierigkeiten beim Export oder Transfer von Trainingsdaten und Modellen zwischen Plattformen.\n‚Ä¢ Skill Lock-in: Aufbau von Expertise in anbieterspezifischen Tools, die nicht √ºbertragbar sind.\n‚Ä¢ Economic Lock-in: Hohe Wechselkosten durch Investitionen in spezifische Technologien oder Vertr√§ge.\n\nüèóÔ∏è ADVISORI's Vendor-Agnostic Architecture Strategy:\n‚Ä¢ Multi-Cloud und Hybrid Approaches: Implementierung von Architekturen, die mehrere Cloud-Anbieter und On-Premise-L√∂sungen kombinieren.\n‚Ä¢ Standardized APIs und Interfaces: Verwendung offener Standards und Abstraktionsschichten zur Entkopplung von spezifischen Anbietern.\n‚Ä¢ Containerization und Orchestration: Einsatz von Container-Technologien f√ºr portable KI-Workloads.\n‚Ä¢ Open Source Integration: Strategische Nutzung von Open-Source-Technologien zur Reduzierung der Anbieterabh√§ngigkeit.\n\nüìä Strategic Vendor Management:\n‚Ä¢ Vendor Diversification: Aufbau von Beziehungen zu mehreren KI-Anbietern zur Risikominimierung.\n‚Ä¢ Negotiation Strategies: Verhandlung flexibler Vertr√§ge mit Exit-Klauseln und Datenportabilit√§t.\n‚Ä¢ Performance Benchmarking: Kontinuierliche Bewertung verschiedener Anbieter zur Aufrechterhaltung von Alternativen.\n‚Ä¢ Technology Roadmap Alignment: Sicherstellung, dass Anbieter-Roadmaps mit Unternehmenszielen √ºbereinstimmen.\n\nüîÑ Migration und Portability Planning:\n‚Ä¢ Data Portability Frameworks: Entwicklung von Strategien f√ºr den nahtlosen Transfer von Daten und Modellen.\n‚Ä¢ Migration Testing: Regelm√§√üige Tests der Migrationsf√§higkeit zu alternativen Plattformen.\n‚Ä¢ Backup Strategies: Implementierung von Backup-L√∂sungen f√ºr kritische KI-Funktionen.\n‚Ä¢ Gradual Transition Plans: Entwicklung schrittweiser Migrationspl√§ne zur Risikominimierung.\n\nüí° Innovation und Future-Proofing:\n‚Ä¢ Technology Scouting: Kontinuierliche √úberwachung neuer KI-Technologien und -Anbieter.\n‚Ä¢ Proof of Concept Programs: Regelm√§√üige Evaluierung alternativer L√∂sungen durch Pilotprojekte.\n‚Ä¢ Internal Capability Building: Aufbau interner KI-Kompetenzen zur Reduzierung der Anbieterabh√§ngigkeit.\n‚Ä¢ Strategic Partnerships: Entwicklung strategischer Partnerschaften, die Flexibilit√§t und Innovation f√∂rdern.\n\nüìà Risk Mitigation und Governance:\n‚Ä¢ Vendor Risk Assessment: Umfassende Bewertung der finanziellen Stabilit√§t und strategischen Ausrichtung von KI-Anbietern.\n‚Ä¢ Contingency Planning: Entwicklung von Notfallpl√§nen f√ºr verschiedene Vendor-Ausfallszenarien.\n‚Ä¢ Legal Safeguards: Implementierung rechtlicher Schutzma√ünahmen in Vendor-Vertr√§gen.\n‚Ä¢ Regular Review Cycles: Etablierung regelm√§√üiger √úberpr√ºfungen der Vendor-Strategie und -Performance."
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQs batch 3 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
