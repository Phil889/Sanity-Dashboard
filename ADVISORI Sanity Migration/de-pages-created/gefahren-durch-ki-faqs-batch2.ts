import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating Gefahren durch KI page with FAQs batch 2...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'gefahren-durch-ki' })
    
    if (!existingDoc) {
      throw new Error('Document "gefahren-durch-ki" not found')
    }
    
    // Create new FAQs
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 5),
        question: 'Wie k√∂nnen Model Extraction Angriffe unser geistiges Eigentum gef√§hrden und welche Schutzstrategien implementiert ADVISORI?',
        answer: "Model Extraction stellt eine der subtilsten und gleichzeitig gef√§hrlichsten Bedrohungen f√ºr Unternehmen dar, die propriet√§re KI-Modelle entwickelt haben. Diese Angriffe zielen darauf ab, durch gezielte Abfragen die Funktionalit√§t und das Wissen eines KI-Modells zu rekonstruieren, ohne direkten Zugang zum urspr√ºnglichen Code oder den Trainingsdaten zu haben. F√ºr Unternehmen bedeutet dies den potenziellen Verlust von Millionen-Investitionen in Forschung und Entwicklung sowie strategischen Wettbewerbsvorteilen. ADVISORI entwickelt mehrschichtige Schutzstrategien, die sowohl technische als auch rechtliche Aspekte des IP-Schutzes umfassen.\n\nüîç Model Extraction Angriffsvektoren und Gesch√§ftsrisiken:\n‚Ä¢ Query-based Extraction: Systematische Abfrage von KI-APIs zur Rekonstruktion der Modelllogik und Entscheidungsgrenzen.\n‚Ä¢ Membership Inference: Bestimmung, ob spezifische Daten im Trainingssatz enthalten waren, um R√ºckschl√ºsse auf propriet√§re Datenquellen zu ziehen.\n‚Ä¢ Property Inference: Ableitung von Modellarchitektur, Hyperparametern und Trainingsprozessen durch Analyse der Modellresponses.\n‚Ä¢ Functional Extraction: Entwicklung von Surrogatmodellen, die √§hnliche Funktionalit√§t wie das urspr√ºngliche Modell bieten.\n\nüõ°Ô∏è ADVISORI's Comprehensive IP Protection Framework:\n‚Ä¢ Query Rate Limiting und Behavioral Analysis: Implementierung intelligenter √úberwachungssysteme, die verd√§chtige Abfragemuster erkennen und automatisch Schutzma√ünahmen aktivieren.\n‚Ä¢ Differential Privacy f√ºr Model Outputs: Einf√ºhrung kontrollierter Unsch√§rfe in Modellantworten, die die Funktionalit√§t erh√§lt, aber Extraction erschwert.\n‚Ä¢ Watermarking und Fingerprinting: Einbettung eindeutiger Identifikatoren in Modellverhalten, die bei unbefugter Nutzung nachweisbar sind.\n‚Ä¢ API Security und Access Control: Entwicklung robuster Authentifizierungs- und Autorisierungssysteme mit granularer Zugriffskontrolle.\n\nüîí Advanced Protection Mechanisms:\n‚Ä¢ Model Obfuscation: Verschleierung der Modellarchitektur und -parameter durch Ensemble-Methoden und Distillation-Techniken.\n‚Ä¢ Adversarial Perturbations: Gezielte Einf√ºhrung von St√∂rungen, die f√ºr legitime Nutzer unsichtbar sind, aber Extraction-Versuche behindern.\n‚Ä¢ Honeypot Queries: Implementierung von Fallen, die Extraction-Versuche erkennen und Angreifer identifizieren.\n‚Ä¢ Secure Multi-Party Computation: Erm√∂glichung von KI-Inferenz ohne Preisgabe des Modells an externe Parteien.\n\nüìä Legal und Business Continuity Measures:\n‚Ä¢ IP Documentation und Patent Strategy: Umfassende Dokumentation der Modellentwicklung und strategische Patentanmeldungen zum rechtlichen Schutz.\n‚Ä¢ Licensing und Usage Agreements: Entwicklung wasserdichter Nutzungsvereinbarungen mit klaren Sanktionen bei Missbrauch.\n‚Ä¢ Incident Response und Forensics: Etablierung spezialisierter Verfahren zur Erkennung, Dokumentation und rechtlichen Verfolgung von IP-Diebstahl.\n‚Ä¢ Insurance und Risk Transfer: Bewertung und Absicherung von IP-Risiken durch spezialisierte Cyber-Versicherungen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 6),
        question: 'Welche spezifischen Risiken entstehen durch Bias und Fairness-Probleme in KI-Systemen und wie adressiert ADVISORI diese ethischen Herausforderungen?',
        answer: "Bias und Fairness-Probleme in KI-Systemen stellen nicht nur ethische Herausforderungen dar, sondern k√∂nnen zu erheblichen rechtlichen, finanziellen und reputativen Risiken f√ºr Unternehmen f√ºhren. Diskriminierende KI-Entscheidungen k√∂nnen zu Klagen, regulatorischen Sanktionen und dauerhaften Sch√§den am Markenimage f√ºhren. ADVISORI versteht Fairness als fundamentalen Baustein vertrauensvoller KI-Systeme und entwickelt umfassende Frameworks zur Erkennung, Messung und Minimierung von Bias in allen Phasen des KI-Lebenszyklus.\n\n‚öñÔ∏è Bias-Kategorien und Gesch√§ftsrisiken:\n‚Ä¢ Historical Bias: Verst√§rkung gesellschaftlicher Vorurteile durch historische Trainingsdaten, die zu systematischer Diskriminierung f√ºhren k√∂nnen.\n‚Ä¢ Representation Bias: Unausgewogene Datenrepr√§sentation bestimmter Gruppen, die zu unfairen Behandlungen f√ºhrt.\n‚Ä¢ Measurement Bias: Systematische Fehler in Datensammlung oder -labeling, die Modellentscheidungen verzerren.\n‚Ä¢ Algorithmic Bias: Inh√§rente Verzerrungen in Algorithmus-Design oder Feature-Selection, die bestimmte Gruppen benachteiligen.\n\nüîç ADVISORI's Comprehensive Bias Detection Framework:\n‚Ä¢ Multi-dimensional Fairness Metrics: Implementierung verschiedener Fairness-Definitionen und Metriken zur umfassenden Bewertung von Modellverhalten.\n‚Ä¢ Intersectional Analysis: Untersuchung von Bias-Effekten an der Schnittstelle mehrerer demografischer Merkmale.\n‚Ä¢ Counterfactual Fairness Testing: Analyse hypothetischer Szenarien zur Identifikation versteckter Diskriminierungsmuster.\n‚Ä¢ Continuous Bias Monitoring: Langfristige √úberwachung von Modellentscheidungen zur Fr√ºherkennung von Bias-Drift.\n\nüõ†Ô∏è Proactive Bias Mitigation Strategies:\n‚Ä¢ Data Augmentation und Synthetic Data: Generierung ausgewogener Trainingsdaten zur Kompensation historischer Verzerrungen.\n‚Ä¢ Fairness-Constrained Learning: Entwicklung von Trainingsverfahren, die Fairness-Constraints direkt in die Optimierung integrieren.\n‚Ä¢ Post-processing Calibration: Nachtr√§gliche Anpassung von Modelloutputs zur Gew√§hrleistung fairer Behandlung aller Gruppen.\n‚Ä¢ Adversarial Debiasing: Einsatz adversarialer Techniken zur Entfernung diskriminierender Informationen aus Modellrepr√§sentationen.\n\nüèõÔ∏è Governance und Compliance Framework:\n‚Ä¢ Ethics Review Boards: Etablierung interdisziplin√§rer Gremien zur ethischen Bewertung von KI-Projekten.\n‚Ä¢ Algorithmic Impact Assessments: Systematische Bewertung potenzieller gesellschaftlicher Auswirkungen vor Modelldeployment.\n‚Ä¢ Transparency und Explainability: Entwicklung interpretierbarer Modelle, die Entscheidungsprozesse nachvollziehbar machen.\n‚Ä¢ Stakeholder Engagement: Einbindung betroffener Gemeinschaften und Interessengruppen in den Entwicklungsprozess.\n\nüìà Business Value und Risk Management:\n‚Ä¢ Regulatory Compliance: Proaktive Einhaltung entstehender AI-Regulierungen und Anti-Diskriminierungsgesetze.\n‚Ä¢ Brand Protection: Schutz der Unternehmensreputation durch verantwortungsvolle KI-Praktiken.\n‚Ä¢ Market Access: Sicherstellung der Marktf√§higkeit von KI-Produkten in verschiedenen Jurisdiktionen und Kulturen.\n‚Ä¢ Innovation Enablement: Schaffung von Vertrauen bei Kunden und Partnern durch nachweislich faire KI-Systeme."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 7),
        question: 'Wie sch√ºtzt ADVISORI vor Supply Chain Attacks auf KI-Systeme und welche Risiken entstehen durch kompromittierte ML-Bibliotheken?',
        answer: "Supply Chain Attacks auf KI-Systeme stellen eine wachsende und besonders heimt√ºckische Bedrohung dar, da sie die Vertrauenskette zwischen Entwicklern und den von ihnen verwendeten Tools, Bibliotheken und Datenquellen ausnutzen. Diese Angriffe k√∂nnen bereits in fr√ºhen Entwicklungsphasen erfolgen und bleiben oft lange unentdeckt, w√§hrend sie systematisch Schwachstellen oder Backdoors in KI-Systeme einschleusen. ADVISORI entwickelt umfassende Supply Chain Security Frameworks, die jeden Aspekt der KI-Entwicklungskette absichern.\n\nüîó Supply Chain Angriffsvektoren in der KI-Entwicklung:\n‚Ä¢ Compromised ML Libraries: Manipulation popul√§rer Machine Learning-Bibliotheken wie TensorFlow, PyTorch oder scikit-learn durch Einschleusung malicious Code.\n‚Ä¢ Poisoned Pre-trained Models: Kontamination √∂ffentlich verf√ºgbarer vortrainierter Modelle mit versteckten Backdoors oder Bias.\n‚Ä¢ Malicious Datasets: Bereitstellung manipulierter Trainingsdaten √ºber vermeintlich vertrauensw√ºrdige Quellen.\n‚Ä¢ Development Tool Compromise: Angriffe auf Entwicklungsumgebungen, IDEs oder CI/CD-Pipelines zur Manipulation des Build-Prozesses.\n\nüõ°Ô∏è ADVISORI's Multi-Layer Supply Chain Security:\n‚Ä¢ Dependency Scanning und Vulnerability Management: Kontinuierliche √úberwachung aller verwendeten Bibliotheken und Frameworks auf bekannte Schwachstellen und verd√§chtige √Ñnderungen.\n‚Ä¢ Code Signing und Integrity Verification: Implementierung kryptographischer Verfahren zur Verifikation der Authentizit√§t und Integrit√§t aller Softwarekomponenten.\n‚Ä¢ Isolated Build Environments: Verwendung containerisierter und isolierter Entwicklungsumgebungen zur Minimierung von Kontaminationsrisiken.\n‚Ä¢ Vendor Risk Assessment: Umfassende Bewertung und kontinuierliche √úberwachung aller Technologie-Lieferanten und Open-Source-Projekte.\n\nüîç Advanced Threat Detection und Monitoring:\n‚Ä¢ Behavioral Analysis von Dependencies: √úberwachung des Verhaltens verwendeter Bibliotheken zur Erkennung unerwarteter oder verd√§chtiger Aktivit√§ten.\n‚Ä¢ Supply Chain Threat Intelligence: Integration spezialisierter Threat Intelligence zur Fr√ºherkennung von Kompromittierungen in der ML-Community.\n‚Ä¢ Automated Security Testing: Implementierung automatisierter Sicherheitstests f√ºr alle externen Abh√§ngigkeiten vor deren Integration.\n‚Ä¢ Provenance Tracking: Vollst√§ndige Nachverfolgung der Herkunft und Verarbeitungsgeschichte aller verwendeten Komponenten.\n\nüèóÔ∏è Secure Development Lifecycle Integration:\n‚Ä¢ Zero Trust Architecture: Implementierung von Zero-Trust-Prinzipien, bei denen keine Komponente automatisch als vertrauensw√ºrdig betrachtet wird.\n‚Ä¢ Least Privilege Access: Minimierung der Berechtigungen f√ºr alle Entwicklungstools und -prozesse.\n‚Ä¢ Secure Defaults und Hardening: Konfiguration aller Systeme mit sicherheitsorientierten Standardeinstellungen.\n‚Ä¢ Regular Security Audits: Durchf√ºhrung regelm√§√üiger Sicherheits√ºberpr√ºfungen der gesamten Entwicklungsinfrastruktur.\n\nüìä Incident Response und Recovery:\n‚Ä¢ Supply Chain Incident Response Plan: Spezialisierte Verfahren zur schnellen Reaktion auf Supply Chain Kompromittierungen.\n‚Ä¢ Rollback und Recovery Procedures: Etablierung schneller Wiederherstellungsverfahren bei Entdeckung kompromittierter Komponenten.\n‚Ä¢ Forensic Capabilities: Entwicklung von F√§higkeiten zur detaillierten Analyse und Nachverfolgung von Supply Chain Angriffen.\n‚Ä¢ Stakeholder Communication: Vorbereitung transparenter Kommunikationsstrategien f√ºr den Fall von Supply Chain Vorf√§llen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 8),
        question: 'Welche Rolle spielen Insider Threats bei KI-Sicherheit und wie implementiert ADVISORI Schutzma√ünahmen gegen interne Bedrohungen?',
        answer: "Insider Threats stellen eine der komplexesten und schwer zu erkennenden Bedrohungen f√ºr KI-Systeme dar, da sie von Personen ausgehen, die bereits autorisierten Zugang zu kritischen Systemen und Daten haben. Bei KI-Systemen sind die Risiken besonders hoch, da Insider Zugang zu wertvollen Trainingsdaten, propriet√§ren Algorithmen und sensiblen Modellparametern haben k√∂nnen. ADVISORI entwickelt umfassende Insider Threat Detection und Prevention Frameworks, die technische √úberwachung mit organisatorischen Ma√ünahmen kombinieren.\n\nüë§ Insider Threat Kategorien in KI-Umgebungen:\n‚Ä¢ Malicious Insiders: Mitarbeiter oder Auftragnehmer, die bewusst Sch√§den verursachen oder geistiges Eigentum stehlen wollen.\n‚Ä¢ Compromised Insiders: Legitime Nutzer, deren Accounts oder Ger√§te von externen Angreifern kompromittiert wurden.\n‚Ä¢ Negligent Insiders: Mitarbeiter, die durch Fahrl√§ssigkeit oder mangelndes Sicherheitsbewusstsein Risiken schaffen.\n‚Ä¢ Privileged User Abuse: Missbrauch von administrativen oder entwicklungsbezogenen Privilegien f√ºr unbefugte Aktivit√§ten.\n\nüîç ADVISORI's Behavioral Analytics Framework:\n‚Ä¢ User and Entity Behavior Analytics: Kontinuierliche √úberwachung von Nutzerverhalten zur Erkennung von Anomalien und verd√§chtigen Aktivit√§ten.\n‚Ä¢ Data Access Pattern Analysis: Analyse von Datenzugriffsmustern zur Identifikation ungew√∂hnlicher oder unbefugter Datennutzung.\n‚Ä¢ Model Interaction Monitoring: √úberwachung der Interaktionen mit KI-Modellen zur Erkennung von Extraction-Versuchen oder Manipulation.\n‚Ä¢ Privilege Escalation Detection: Erkennung von Versuchen, Berechtigungen zu erweitern oder auf nicht autorisierte Ressourcen zuzugreifen.\n\nüõ°Ô∏è Technical Safeguards und Access Controls:\n‚Ä¢ Zero Trust Architecture: Implementierung von Zero-Trust-Prinzipien mit kontinuierlicher Verifikation aller Nutzer und Ger√§te.\n‚Ä¢ Least Privilege Access: Minimierung von Berechtigungen auf das absolut notwendige Ma√ü f√ºr jede Rolle und Aufgabe.\n‚Ä¢ Multi-Factor Authentication: Starke Authentifizierung f√ºr alle Zugriffe auf kritische KI-Systeme und Daten.\n‚Ä¢ Data Loss Prevention: Implementierung von DLP-Systemen zur Verhinderung unbefugter Datenexfiltration.\n\nüè¢ Organizational und Cultural Measures:\n‚Ä¢ Security Awareness Training: Regelm√§√üige Schulungen zu KI-spezifischen Sicherheitsrisiken und Best Practices.\n‚Ä¢ Background Checks und Vetting: Umfassende √úberpr√ºfung von Mitarbeitern mit Zugang zu kritischen KI-Systemen.\n‚Ä¢ Separation of Duties: Aufteilung kritischer Aufgaben auf mehrere Personen zur Verhinderung von Einzelpunkt-Risiken.\n‚Ä¢ Whistleblower Programs: Etablierung sicherer Kan√§le f√ºr die Meldung verd√§chtiger Aktivit√§ten.\n\nüìä Monitoring und Response Capabilities:\n‚Ä¢ Real-time Alert Systems: Sofortige Benachrichtigung bei Erkennung verd√§chtiger Insider-Aktivit√§ten.\n‚Ä¢ Forensic Data Collection: Kontinuierliche Sammlung von Audit-Daten f√ºr detaillierte Untersuchungen.\n‚Ä¢ Automated Response Actions: Automatische Sperrung oder Einschr√§nkung von Accounts bei Erkennung kritischer Bedrohungen.\n‚Ä¢ Legal und HR Coordination: Enge Zusammenarbeit mit Rechts- und Personalabteilungen bei Insider Threat Incidents."
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQs batch 2 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
