import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating Beratung KI-Sicherheit page with FAQs batch 2...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'beratung-ki-sicherheit' })
    
    if (!existingDoc) {
      throw new Error('Document "beratung-ki-sicherheit" not found')
    }
    
    // Create new FAQs
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 5),
        question: 'Wie k√∂nnen Unternehmen ihre AI-Modelle vor Data Poisoning und Model Manipulation sch√ºtzen und welche Erkennungsverfahren empfiehlt ADVISORI?',
        answer: "Data Poisoning und Model Manipulation geh√∂ren zu den heimt√ºckischsten Bedrohungen f√ºr AI-Systeme, da sie oft unbemerkt bleiben und langfristige Sch√§den verursachen k√∂nnen. Diese Angriffe zielen darauf ab, die Integrit√§t von Trainingsdaten oder Modellen zu kompromittieren, um das Verhalten des AI-Systems zu manipulieren. ADVISORI entwickelt mehrschichtige Schutzstrategien, die sowohl pr√§ventive als auch detektive Ma√ünahmen umfassen.\n\nüîç Comprehensive Data Integrity Protection:\n‚Ä¢ Data Provenance Tracking: Implementierung l√ºckenloser Nachverfolgung der Datenherkunft und -verarbeitung, um manipulierte oder kompromittierte Datenquellen zu identifizieren.\n‚Ä¢ Statistical Anomaly Detection: Einsatz fortschrittlicher statistischer Verfahren zur Erkennung ungew√∂hnlicher Muster in Trainingsdaten, die auf Poisoning-Angriffe hindeuten k√∂nnten.\n‚Ä¢ Cryptographic Data Validation: Verwendung kryptographischer Signaturen und Hashing-Verfahren zur Gew√§hrleistung der Datenintegrit√§t w√§hrend des gesamten ML-Lebenszyklus.\n‚Ä¢ Multi-Source Validation: Kreuzvalidierung von Trainingsdaten aus verschiedenen unabh√§ngigen Quellen zur Identifikation inkonsistenter oder manipulierter Informationen.\n\nüõ°Ô∏è ADVISORI's Model Protection Framework:\n‚Ä¢ Secure Model Training: Implementierung isolierter und √ºberwachter Trainingsumgebungen, die unbefugten Zugriff auf Modelle und Trainingsprozesse verhindern.\n‚Ä¢ Model Versioning und Integrity Checks: Systematische Versionierung von AI-Modellen mit kryptographischen Integrit√§tspr√ºfungen zur Erkennung unautorisierten Modifikationen.\n‚Ä¢ Behavioral Baseline Monitoring: Kontinuierliche √úberwachung des Modellverhaltens gegen etablierte Baselines zur fr√ºhzeitigen Erkennung von Anomalien oder Manipulationen.\n‚Ä¢ Federated Learning Security: Spezielle Sicherheitsma√ünahmen f√ºr dezentrale Lernszenarien, um Poisoning-Angriffe in verteilten Umgebungen zu verhindern."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 6),
        question: 'Welche spezifischen Sicherheitsherausforderungen entstehen beim Deployment von AI-Modellen in Produktionsumgebungen und wie adressiert ADVISORI diese?',
        answer: "Das Deployment von AI-Modellen in Produktionsumgebungen bringt einzigartige Sicherheitsherausforderungen mit sich, die √ºber traditionelle Software-Deployment-Risiken hinausgehen. AI-Systeme in Produktion sind dynamischen Bedrohungen ausgesetzt und m√ºssen gleichzeitig Performance, Sicherheit und Compliance gew√§hrleisten. ADVISORI entwickelt spezialisierte Deployment-Strategien, die diese komplexen Anforderungen erf√ºllen.\n\nüöÄ Production AI Security Challenges:\n‚Ä¢ Model Drift und Performance Degradation: Kontinuierliche √úberwachung der Modellleistung zur Erkennung von Concept Drift oder schleichender Performance-Verschlechterung, die Sicherheitsl√ºcken schaffen k√∂nnte.\n‚Ä¢ Real-time Threat Detection: Implementierung von Echtzeit-Monitoring-Systemen, die verd√§chtige Eingaben oder Anomalien im Modellverhalten sofort erkennen und darauf reagieren.\n‚Ä¢ Scalability und Security Trade-offs: Balancierung zwischen Performance-Anforderungen und Sicherheitsma√ünahmen in hochskalierten Produktionsumgebungen.\n‚Ä¢ API Security und Access Control: Absicherung von AI-Model-APIs gegen unbefugten Zugriff, Missbrauch und Reverse Engineering-Versuche.\n\nüîí ADVISORI's Secure Deployment Architecture:\n‚Ä¢ Zero-Trust AI Infrastructure: Implementierung von Zero-Trust-Prinzipien f√ºr AI-Infrastrukturen, bei denen jede Komponente kontinuierlich validiert und √ºberwacht wird.\n‚Ä¢ Containerized Security: Einsatz sicherer Container-Technologien mit spezialisierten Security-Policies f√ºr AI-Workloads und Isolation kritischer Modellkomponenten.\n‚Ä¢ Automated Security Testing: Integration automatisierter Sicherheitstests in CI/CD-Pipelines f√ºr AI-Modelle, einschlie√ülich Adversarial Testing und Vulnerability Scanning.\n‚Ä¢ Incident Response Automation: Entwicklung automatisierter Reaktionsmechanismen f√ºr Sicherheitsvorf√§lle, die schnelle Isolation und Wiederherstellung kompromittierter AI-Systeme erm√∂glichen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 7),
        question: 'Wie implementiert ADVISORI Explainable AI und Transparency-Mechanismen als Sicherheitsfeatures f√ºr kritische Gesch√§ftsentscheidungen?',
        answer: "Explainable AI ist nicht nur eine ethische Anforderung, sondern ein kritisches Sicherheitsfeature, das Transparenz, Vertrauen und Nachvollziehbarkeit in AI-gest√ºtzten Gesch√§ftsentscheidungen gew√§hrleistet. ADVISORI versteht Explainability als fundamentalen Baustein f√ºr sichere und verantwortungsvolle AI-Implementierungen, der sowohl technische Robustheit als auch regulatorische Compliance erm√∂glicht.\n\nüîç Explainability als Security Layer:\n‚Ä¢ Decision Audit Trails: Implementierung umfassender Audit-Mechanismen, die jeden Schritt der AI-Entscheidungsfindung dokumentieren und nachvollziehbar machen.\n‚Ä¢ Bias Detection und Mitigation: Einsatz von Explainability-Tools zur Identifikation und Korrektur von Bias in AI-Modellen, der zu diskriminierenden oder fehlerhaften Entscheidungen f√ºhren k√∂nnte.\n‚Ä¢ Anomaly Explanation: Entwicklung von Systemen, die nicht nur Anomalien erkennen, sondern auch verst√§ndliche Erkl√§rungen f√ºr ungew√∂hnliche AI-Entscheidungen liefern.\n‚Ä¢ Stakeholder Communication: Schaffung von Mechanismen zur verst√§ndlichen Kommunikation von AI-Entscheidungen an verschiedene Stakeholder-Gruppen.\n\nüí° ADVISORI's Transparency Framework:\n‚Ä¢ Multi-Level Explainability: Implementierung verschiedener Erkl√§rungsebenen, von technischen Details f√ºr Entwickler bis zu verst√§ndlichen Zusammenfassungen f√ºr Gesch√§ftsnutzer.\n‚Ä¢ Real-time Explanation Generation: Entwicklung von Systemen, die in Echtzeit verst√§ndliche Erkl√§rungen f√ºr AI-Entscheidungen generieren, ohne die Performance zu beeintr√§chtigen.\n‚Ä¢ Regulatory Compliance Integration: Anpassung von Explainability-Mechanismen an spezifische regulatorische Anforderungen verschiedener Branchen und Jurisdiktionen.\n‚Ä¢ Interactive Explanation Interfaces: Schaffung benutzerfreundlicher Interfaces, die es Stakeholdern erm√∂glichen, AI-Entscheidungen zu verstehen und bei Bedarf zu hinterfragen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 8),
        question: 'Welche Rolle spielt Continuous Security Monitoring bei AI-Systemen und wie etabliert ADVISORI effektive √úberwachungsstrategien?',
        answer: "Continuous Security Monitoring ist f√ºr AI-Systeme noch kritischer als f√ºr traditionelle IT-Infrastrukturen, da AI-Modelle dynamisch lernen und sich entwickeln, was neue Sicherheitsrisiken schaffen kann. ADVISORI entwickelt adaptive Monitoring-Strategien, die sowohl technische Performance als auch Sicherheitsaspekte kontinuierlich √ºberwachen und proaktiv auf Bedrohungen reagieren.\n\nüìä AI-Specific Monitoring Dimensions:\n‚Ä¢ Model Performance Tracking: Kontinuierliche √úberwachung von Modellgenauigkeit, Latenz und Ressourcenverbrauch zur Erkennung von Performance-Anomalien, die auf Sicherheitsprobleme hindeuten k√∂nnten.\n‚Ä¢ Input Data Quality Monitoring: Real-time Analyse eingehender Daten auf Qualit√§t, Integrit√§t und potenzielle Manipulationsversuche.\n‚Ä¢ Behavioral Pattern Analysis: √úberwachung von AI-Entscheidungsmustern zur Identifikation ungew√∂hnlicher oder verd√§chtiger Verhaltensweisen.\n‚Ä¢ Security Event Correlation: Integration von AI-spezifischen Security Events in bestehende SIEM-Systeme f√ºr ganzheitliche Bedrohungserkennung.\n\nüîÑ ADVISORI's Adaptive Monitoring Architecture:\n‚Ä¢ Machine Learning f√ºr Security Monitoring: Einsatz von ML-Algorithmen zur automatischen Erkennung von Sicherheitsanomalien und zur kontinuierlichen Verbesserung der Monitoring-Effektivit√§t.\n‚Ä¢ Multi-Dimensional Alerting: Implementierung intelligenter Alerting-Systeme, die verschiedene Sicherheitsindikatoren korrelieren und False Positives minimieren.\n‚Ä¢ Automated Response Mechanisms: Entwicklung automatisierter Reaktionssysteme, die bei erkannten Bedrohungen sofortige Schutzma√ünahmen einleiten k√∂nnen.\n‚Ä¢ Compliance Monitoring Integration: Kontinuierliche √úberwachung der Einhaltung von Datenschutz- und Compliance-Anforderungen in AI-Systemen."
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQs batch 2 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
