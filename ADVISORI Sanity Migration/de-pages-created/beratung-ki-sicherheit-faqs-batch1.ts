import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating Beratung KI-Sicherheit page with FAQs batch 1...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'beratung-ki-sicherheit' })
    
    if (!existingDoc) {
      throw new Error('Document "beratung-ki-sicherheit" not found')
    }
    
    // Create new FAQs
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 1),
        question: 'Warum ist KI-Sicherheit mehr als nur traditionelle Cybersecurity und wie adressiert ADVISORI die einzigartigen Herausforderungen von AI-Systemen?',
        answer: "KI-Sicherheit unterscheidet sich fundamental von herk√∂mmlicher Cybersecurity, da AI-Systeme v√∂llig neue Angriffsvektoren und Schwachstellen einf√ºhren, die mit traditionellen Sicherheitsma√ünahmen nicht abgedeckt werden k√∂nnen. W√§hrend klassische IT-Sicherheit prim√§r auf den Schutz von Daten und Systemen vor externen Bedrohungen fokussiert, m√ºssen AI-Sicherheitsstrategien auch die inh√§renten Risiken intelligenter Algorithmen, Modellmanipulation und unvorhersehbare Systemverhalten ber√ºcksichtigen.\n\nüéØ Einzigartige AI-Sicherheitsherausforderungen:\n‚Ä¢ Adversarial Attacks: Gezielte Manipulation von Eingabedaten, um AI-Modelle zu t√§uschen oder falsche Entscheidungen zu provozieren, ohne dass traditionelle Sicherheitssysteme diese Angriffe erkennen.\n‚Ä¢ Model Poisoning: Kompromittierung der Trainingsdaten oder des Lernprozesses, um das Verhalten des AI-Systems dauerhaft zu beeinflussen und Backdoors zu implementieren.\n‚Ä¢ Data Leakage: Unbeabsichtigte Preisgabe sensibler Informationen durch AI-Modelle, die w√§hrend des Trainings auf vertrauliche Daten zugegriffen haben.\n‚Ä¢ Explainability und Transparenz: Schwierigkeit, die Entscheidungsfindung komplexer AI-Systeme nachzuvollziehen und potenzielle Sicherheitsl√ºcken zu identifizieren.\n\nüõ°Ô∏è ADVISORI's ganzheitlicher AI-Security-Ansatz:\n‚Ä¢ Multi-Layer Defense Architecture: Implementierung spezialisierter Sicherheitsschichten, die sowohl traditionelle als auch AI-spezifische Bedrohungen abwehren.\n‚Ä¢ Proaktive Threat Modeling: Entwicklung umfassender Bedrohungsmodelle, die alle Phasen des AI-Lebenszyklus von der Datensammlung bis zum Deployment abdecken.\n‚Ä¢ Continuous Security Validation: Etablierung kontinuierlicher √úberwachungs- und Validierungsprozesse f√ºr AI-Modelle in Produktionsumgebungen.\n‚Ä¢ DSGVO-Integration: Nahtlose Integration von Datenschutzanforderungen in AI-Sicherheitsarchitekturen f√ºr vollst√§ndige Compliance."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 2),
        question: 'Wie k√∂nnen Unternehmen ihre bestehenden AI-Systeme gegen Adversarial Attacks absichern und welche pr√§ventiven Ma√ünahmen empfiehlt ADVISORI?',
        answer: "Adversarial Attacks stellen eine der sophistiziertesten Bedrohungen f√ºr AI-Systeme dar, da sie die fundamentalen Schw√§chen maschineller Lernalgorithmen ausnutzen. Diese Angriffe k√∂nnen bestehende AI-Systeme kompromittieren, ohne dass herk√∂mmliche Sicherheitsma√ünahmen Alarm schlagen. ADVISORI entwickelt mehrschichtige Verteidigungsstrategien, die sowohl reaktive als auch proaktive Schutzma√ünahmen kombinieren.\n\nüîç Comprehensive Adversarial Defense Strategy:\n‚Ä¢ Input Sanitization und Validation: Implementierung robuster Eingabevalidierung, die verd√§chtige oder manipulierte Daten erkennt, bevor sie das AI-Modell erreichen.\n‚Ä¢ Adversarial Training: Systematisches Training von AI-Modellen mit adversarialen Beispielen, um ihre Robustheit gegen bekannte Angriffsmuster zu erh√∂hen.\n‚Ä¢ Ensemble Methods: Einsatz mehrerer AI-Modelle mit unterschiedlichen Architekturen, um die Wahrscheinlichkeit erfolgreicher Angriffe zu reduzieren.\n‚Ä¢ Real-time Anomaly Detection: Kontinuierliche √úberwachung von Modellverhalten und -ausgaben zur Erkennung ungew√∂hnlicher Muster oder Abweichungen.\n\nüõ†Ô∏è ADVISORI's Pr√§ventive Schutzma√ünahmen:\n‚Ä¢ Model Hardening: Systematische St√§rkung von AI-Modellen durch spezialisierte Trainingsmethoden und Architekturoptimierungen.\n‚Ä¢ Defense-in-Depth Architecture: Implementierung mehrschichtiger Sicherheitsarchitekturen, die verschiedene Verteidigungslinien gegen adversariale Angriffe etablieren.\n‚Ä¢ Threat Intelligence Integration: Kontinuierliche Aktualisierung der Verteidigungsstrategien basierend auf neuesten Erkenntnissen √ºber adversariale Angriffstechniken.\n‚Ä¢ Incident Response Planning: Entwicklung spezialisierter Reaktionspl√§ne f√ºr den Fall erfolgreicher adversarialer Angriffe, einschlie√ülich Schadensbegrenzung und Systemwiederherstellung."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 3),
        question: 'Welche DSGVO-spezifischen Anforderungen gelten f√ºr AI-Systeme und wie stellt ADVISORI sicher, dass KI-Implementierungen vollst√§ndig datenschutzkonform sind?',
        answer: "Die DSGVO stellt besondere Herausforderungen f√ºr AI-Systeme dar, da viele traditionelle Datenschutzprinzipien nicht direkt auf maschinelles Lernen anwendbar sind. KI-Systeme verarbeiten oft gro√üe Mengen personenbezogener Daten auf komplexe Weise, was spezielle Compliance-Strategien erfordert. ADVISORI entwickelt ma√ügeschneiderte DSGVO-Compliance-Frameworks, die sowohl rechtliche Anforderungen erf√ºllen als auch die Innovationskraft der KI erhalten.\n\nüìã DSGVO-Kernprinzipien f√ºr AI-Systeme:\n‚Ä¢ Rechtm√§√üigkeit und Transparenz: Etablierung klarer Rechtsgrundlagen f√ºr AI-Datenverarbeitung und Gew√§hrleistung nachvollziehbarer Entscheidungsprozesse durch Explainable AI-Technologien.\n‚Ä¢ Zweckbindung und Datenminimierung: Sicherstellung, dass AI-Systeme nur f√ºr definierte Zwecke eingesetzt werden und ausschlie√ülich notwendige Daten verarbeiten.\n‚Ä¢ Richtigkeit und Speicherbegrenzung: Implementierung von Mechanismen zur Gew√§hrleistung der Datenqualit√§t und automatischen L√∂schung nicht mehr ben√∂tigter Informationen.\n‚Ä¢ Betroffenenrechte: Technische Umsetzung von Auskunfts-, Berichtigungs- und L√∂schungsrechten in AI-Systemen.\n\nüîí ADVISORI's Privacy-by-Design f√ºr AI:\n‚Ä¢ Differential Privacy: Implementierung mathematischer Verfahren, die Datenschutz auf algorithmischer Ebene gew√§hrleisten, ohne die Modellleistung zu beeintr√§chtigen.\n‚Ä¢ Federated Learning: Entwicklung dezentraler Lernans√§tze, die es erm√∂glichen, AI-Modelle zu trainieren, ohne sensible Daten zu zentralisieren.\n‚Ä¢ Data Anonymization: Einsatz fortschrittlicher Anonymisierungstechniken, die auch bei komplexen AI-Anwendungen wirksam bleiben.\n‚Ä¢ Consent Management: Implementierung granularer Einwilligungssysteme, die dynamische Anpassungen der Datenverarbeitung basierend auf Nutzerpr√§ferenzen erm√∂glichen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 4),
        question: 'Wie entwickelt ADVISORI eine umfassende AI Governance-Strategie, die sowohl technische Sicherheit als auch ethische Verantwortung gew√§hrleistet?',
        answer: "AI Governance ist ein multidimensionales Framework, das technische Exzellenz, ethische Verantwortung und regulatorische Compliance in einem koh√§renten System vereint. ADVISORI versteht AI Governance nicht als nachgelagerte Compliance-√úbung, sondern als strategischen Enabler f√ºr verantwortungsvolle Innovation. Unser Ansatz integriert Governance-Prinzipien von der Konzeption bis zur Implementierung und dar√ºber hinaus.\n\nüèõÔ∏è Fundamentale Governance-Dimensionen:\n‚Ä¢ Ethical AI Framework: Entwicklung unternehmensweiter Ethik-Richtlinien, die Fairness, Transparenz und Verantwortlichkeit in allen AI-Anwendungen gew√§hrleisten.\n‚Ä¢ Risk Management Integration: Systematische Integration von AI-Risiken in bestehende Enterprise Risk Management-Systeme und Governance-Strukturen.\n‚Ä¢ Stakeholder Engagement: Etablierung von Prozessen zur Einbindung aller relevanten Stakeholder in AI-Entscheidungen, von Entwicklern bis zu Endnutzern.\n‚Ä¢ Continuous Monitoring: Implementierung kontinuierlicher √úberwachungssysteme f√ºr AI-Performance, Bias-Erkennung und Compliance-Validierung.\n\n‚öñÔ∏è ADVISORI's Responsible AI Implementation:\n‚Ä¢ Multi-Stakeholder Governance Boards: Einrichtung interdisziplin√§rer Gremien, die technische, ethische und gesch√§ftliche Perspektiven in AI-Entscheidungen einbringen.\n‚Ä¢ Algorithmic Auditing: Entwicklung systematischer Audit-Prozesse zur regelm√§√üigen √úberpr√ºfung von AI-Systemen auf Bias, Fairness und Performance.\n‚Ä¢ Transparency Mechanisms: Implementierung von Systemen zur Dokumentation und Kommunikation von AI-Entscheidungen gegen√ºber internen und externen Stakeholdern.\n‚Ä¢ Adaptive Governance Frameworks: Schaffung flexibler Governance-Strukturen, die sich an evolvierende Technologien, Regulierungen und gesellschaftliche Erwartungen anpassen k√∂nnen."
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQs batch 1 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
