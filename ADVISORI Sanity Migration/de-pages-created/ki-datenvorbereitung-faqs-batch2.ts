import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating KI-Datenvorbereitung page with FAQs batch 2...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'ki-datenvorbereitung' })
    
    if (!existingDoc) {
      throw new Error('Document "ki-datenvorbereitung" not found')
    }
    
    // Create new FAQs
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 5),
        question: 'Welche konkreten Herausforderungen l√∂st ADVISORI bei der Implementierung automatisierter Datenvorbereitungspipelines und wie gew√§hrleisten wir Skalierbarkeit ohne Qualit√§tsverluste?',
        answer: "Die Automatisierung von Datenvorbereitungsprozessen ist ein komplexes Unterfangen, das weit √ºber einfache Skriptierung hinausgeht. ADVISORI entwickelt intelligente, selbstadaptive Pipelines, die nicht nur aktuelle Datenanforderungen erf√ºllen, sondern auch flexibel auf sich √§ndernde Gesch√§ftsanforderungen und Datenstrukturen reagieren k√∂nnen, w√§hrend sie gleichzeitig h√∂chste Qualit√§ts- und Compliance-Standards aufrechterhalten.\n\n‚öôÔ∏è Intelligente Automatisierung mit adaptiver Logik:\n‚Ä¢ Selbstlernende Datenvalidierung: Implementierung von Machine Learning-basierten Validierungsalgorithmen, die automatisch Anomalien erkennen und Qualit√§tsstandards kontinuierlich anpassen.\n‚Ä¢ Dynamische Schema-Evolution: Entwicklung von Pipelines, die automatisch auf √Ñnderungen in Datenstrukturen reagieren und sich ohne manuelle Intervention anpassen k√∂nnen.\n‚Ä¢ Kontextuelle Datenbereinigung: Intelligente Bereinigungslogik, die den Gesch√§ftskontext versteht und datenspezifische Bereinigungsstrategien automatisch ausw√§hlt.\n‚Ä¢ Predictive Quality Management: Vorhersage potenzieller Datenqualit√§tsprobleme basierend auf historischen Mustern und proaktive Gegenma√ünahmen.\n\nüîÑ Skalierbarkeits-Framework f√ºr Enterprise-Anforderungen:\n‚Ä¢ Mikroservice-Architektur: Modulare Pipeline-Komponenten, die unabh√§ngig skaliert und optimiert werden k√∂nnen, ohne das Gesamtsystem zu beeintr√§chtigen.\n‚Ä¢ Cloud-native Elastizit√§t: Automatische Ressourcenskalierung basierend auf Datenvolumen und Verarbeitungsanforderungen f√ºr kostenoptimierte Performance.\n‚Ä¢ Parallelisierung und Distributed Computing: Intelligente Verteilung von Datenverarbeitungslasten f√ºr maximale Effizienz bei gro√üen Datenmengen.\n‚Ä¢ Quality-at-Scale Monitoring: Kontinuierliche √úberwachung der Datenqualit√§t auch bei exponentiell wachsenden Datenvolumen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 6),
        question: 'Wie adressiert ADVISORI die komplexen Herausforderungen des Feature Engineering f√ºr verschiedene KI-Anwendungsf√§lle und welche Strategien nutzen wir f√ºr optimale Modell-Performance?',
        answer: "Feature Engineering ist die Kunst und Wissenschaft, aus Rohdaten die wertvollsten Informationen f√ºr KI-Modelle zu extrahieren. ADVISORI verfolgt einen datengetriebenen und dom√§nenspezifischen Ansatz, der sowohl automatisierte Techniken als auch tiefes Gesch√§ftsverst√§ndnis kombiniert, um Features zu entwickeln, die nicht nur statistisch relevant, sondern auch gesch√§ftlich bedeutsam sind.\n\nüß† Strategisches Feature Engineering f√ºr maximale Wertsch√∂pfung:\n‚Ä¢ Domain-Driven Feature Discovery: Entwicklung von Features basierend auf tiefem Verst√§ndnis Ihrer Gesch√§ftsprozesse und Branchendynamiken, nicht nur auf statistischen Korrelationen.\n‚Ä¢ Automated Feature Generation: Einsatz fortschrittlicher Algorithmen zur automatischen Generierung und Bewertung von Feature-Kombinationen f√ºr optimale Modell-Performance.\n‚Ä¢ Temporal und Sequential Features: Spezialisierte Techniken f√ºr zeitbasierte Daten, die Trends, Saisonalit√§t und sequenzielle Muster erfassen.\n‚Ä¢ Cross-Domain Feature Transfer: Wiederverwendung und Anpassung erfolgreicher Features aus √§hnlichen Anwendungsf√§llen f√ºr beschleunigte Entwicklung.\n\nüìä Performance-Optimierung durch intelligente Feature-Selektion:\n‚Ä¢ Multi-Objective Feature Selection: Balancierung zwischen Modell-Genauigkeit, Interpretierbarkeit und Recheneffizienz bei der Feature-Auswahl.\n‚Ä¢ Dimensionalit√§tsreduktion mit Gesch√§ftslogik: Intelligente Reduktion der Feature-Anzahl unter Beibehaltung gesch√§ftskritischer Informationen.\n‚Ä¢ Feature Interaction Mining: Identifikation und Modellierung komplexer Interaktionen zwischen Features f√ºr verbesserte Vorhersagekraft.\n‚Ä¢ Continuous Feature Evolution: Kontinuierliche √úberwachung und Optimierung von Features basierend auf Modell-Performance und sich √§ndernden Gesch√§ftsanforderungen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 7),
        question: 'Welche fortschrittlichen Techniken nutzt ADVISORI f√ºr die Behandlung von unvollst√§ndigen, inkonsistenten oder verrauschten Daten, und wie gew√§hrleisten wir dabei die Integrit√§t der urspr√ºnglichen Informationen?',
        answer: "Reale Daten sind selten perfekt ‚Äì sie enthalten L√ºcken, Inkonsistenzen und Rauschen, die die Qualit√§t von KI-Modellen erheblich beeintr√§chtigen k√∂nnen. ADVISORI entwickelt sophisticated Strategien f√ºr den Umgang mit Datenqualit√§tsproblemen, die nicht nur technische Exzellenz, sondern auch die Bewahrung der urspr√ºnglichen Datenintelligenz und -integrit√§t gew√§hrleisten.\n\nüîç Intelligente Datenbereinigung mit Kontext-Awareness:\n‚Ä¢ Adaptive Imputation Strategies: Entwicklung kontextspezifischer Strategien f√ºr fehlende Werte, die Gesch√§ftslogik und Datenverteilungen ber√ºcksichtigen, anstatt einfache statistische Methoden zu verwenden.\n‚Ä¢ Anomalie-Detection mit Business Logic: Implementierung von Anomalie-Erkennungsalgorithmen, die zwischen echten Ausrei√üern und wertvollen Edge Cases unterscheiden k√∂nnen.\n‚Ä¢ Probabilistic Data Cleaning: Verwendung probabilistischer Modelle zur Sch√§tzung der Wahrscheinlichkeit von Datenfehlern und intelligente Korrekturstrategien.\n‚Ä¢ Temporal Consistency Validation: Spezielle Techniken f√ºr zeitbasierte Daten zur Erkennung und Korrektur von zeitlichen Inkonsistenzen.\n\nüõ°Ô∏è Integrit√§ts-Preservation und Audit-F√§higkeit:\n‚Ä¢ Reversible Data Transformations: Implementierung von Datenbereinigungsprozessen, die bei Bedarf r√ºckg√§ngig gemacht werden k√∂nnen, um urspr√ºngliche Informationen zu bewahren.\n‚Ä¢ Confidence Scoring: Bewertung der Zuverl√§ssigkeit jeder Datenbereinigungsoperation mit Confidence-Scores f√ºr transparente Qualit√§tsbewertung.\n‚Ä¢ Multi-Source Validation: Kreuzvalidierung von Datenbereinigungen durch Abgleich mit externen Datenquellen und Gesch√§ftsregeln.\n‚Ä¢ Comprehensive Data Lineage: Vollst√§ndige Dokumentation aller Bereinigungsschritte f√ºr Audit-Zwecke und Nachvollziehbarkeit."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 8),
        question: 'Wie entwickelt ADVISORI ma√ügeschneiderte Datenvalidierungs- und Qualit√§tssicherungsframeworks, die sowohl technische Exzellenz als auch Gesch√§ftsanforderungen erf√ºllen?',
        answer: "Datenvalidierung ist mehr als nur technische Pr√ºfung ‚Äì sie ist ein strategischer Prozess, der Gesch√§ftslogik, Compliance-Anforderungen und technische Standards harmonisch vereint. ADVISORI entwickelt umfassende Validierungsframeworks, die nicht nur Datenqualit√§t gew√§hrleisten, sondern auch als fr√ºhes Warnsystem f√ºr Gesch√§ftsanomalien und Compliance-Risiken fungieren.\n\n‚úÖ Multi-Layer Validation Architecture:\n‚Ä¢ Business Rule Validation: Implementierung gesch√§ftsspezifischer Validierungsregeln, die √ºber einfache Datentyp-Pr√ºfungen hinausgehen und Gesch√§ftslogik und Branchenstandards ber√ºcksichtigen.\n‚Ä¢ Statistical Quality Monitoring: Kontinuierliche √úberwachung statistischer Eigenschaften der Daten zur Erkennung von Drift, Bias und Qualit√§tsverschlechterung.\n‚Ä¢ Cross-Reference Validation: Validierung durch Abgleich mit externen Referenzdatenquellen und Master Data Management Systemen.\n‚Ä¢ Temporal Validation: Spezielle Pr√ºfungen f√ºr zeitbasierte Konsistenz und logische Sequenzen in historischen Daten.\n\nüéØ Adaptive Quality Frameworks f√ºr dynamische Anforderungen:\n‚Ä¢ Self-Learning Validation Rules: Entwicklung von Validierungsregeln, die sich basierend auf historischen Datenmustern und Gesch√§ftsentwicklungen automatisch anpassen.\n‚Ä¢ Risk-Based Quality Scoring: Priorisierung von Validierungsaktivit√§ten basierend auf Gesch√§ftsrisiko und Auswirkungen auf nachgelagerte KI-Modelle.\n‚Ä¢ Real-Time Quality Dashboards: Entwicklung interaktiver Dashboards f√ºr kontinuierliche √úberwachung der Datenqualit√§t mit actionable Insights f√ºr Gesch√§ftsentscheidungen.\n‚Ä¢ Predictive Quality Analytics: Vorhersage zuk√ºnftiger Datenqualit√§tsprobleme basierend auf Trends und Mustern f√ºr proaktive Qualit√§tssicherung."
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQs batch 2 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
