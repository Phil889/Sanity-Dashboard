import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating Gefahren durch KI page with FAQs batch 1...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'gefahren-durch-ki' })
    
    if (!existingDoc) {
      throw new Error('Document "gefahren-durch-ki" not found')
    }
    
    // Create new FAQs
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 1),
        question: 'Welche konkreten KI-Bedrohungen stellen das gr√∂√üte Risiko f√ºr Unternehmen dar und wie identifiziert ADVISORI diese proaktiv?',
        answer: "Die Bedrohungslandschaft f√ºr KI-Systeme ist komplex und entwickelt sich kontinuierlich weiter. F√ºr C-Level-F√ºhrungskr√§fte ist es entscheidend zu verstehen, dass KI-Gefahren nicht nur technische Risiken darstellen, sondern fundamentale Gesch√§ftsrisiken, die Reputation, Compliance und Wettbewerbsf√§higkeit bedrohen k√∂nnen. ADVISORI verfolgt einen systematischen Ansatz zur Identifikation und Bewertung dieser Bedrohungen, der weit √ºber traditionelle IT-Sicherheit hinausgeht.\n\nüéØ Kritische KI-Bedrohungskategorien:\n‚Ä¢ Adversarial Attacks: Gezielte Manipulation von KI-Eingaben zur T√§uschung von Modellen, die zu falschen Entscheidungen oder Sicherheitsl√ºcken f√ºhren k√∂nnen.\n‚Ä¢ Data Poisoning: Kontamination von Trainingsdaten mit manipulierten Informationen, die die Modellleistung systematisch beeintr√§chtigen oder Backdoors schaffen.\n‚Ä¢ Model Extraction und IP-Diebstahl: Unbefugte Rekonstruktion propriet√§rer KI-Modelle durch gezielte Abfragen oder Reverse Engineering.\n‚Ä¢ Privacy Leakage: Unbeabsichtigte Preisgabe sensibler Trainingsdaten durch Modellinferenz oder Membership Inference Attacks.\n‚Ä¢ Bias Amplification: Verst√§rkung gesellschaftlicher oder gesch√§ftlicher Vorurteile durch unausgewogene Trainingsdaten oder fehlerhafte Algorithmen.\n\nüîç ADVISORI's proaktiver Threat Intelligence Ansatz:\n‚Ä¢ Kontinuierliche Bedrohungsanalyse: Wir √ºberwachen aktuelle Forschung, Sicherheitsvorf√§lle und emerging threats in der KI-Sicherheitslandschaft.\n‚Ä¢ Spezifische Risikomodellierung: Entwicklung ma√ügeschneiderter Bedrohungsmodelle basierend auf Ihrer spezifischen KI-Architektur und Anwendungsf√§llen.\n‚Ä¢ Red Team Assessments: Durchf√ºhrung kontrollierter Angriffssimulationen zur Identifikation von Schwachstellen vor deren Ausnutzung.\n‚Ä¢ Branchenspezifische Threat Intelligence: Ber√ºcksichtigung sektorspezifischer Bedrohungen und regulatorischer Anforderungen.\n\nüõ°Ô∏è Strategische Risikobewertung und Priorisierung:\n‚Ä¢ Business Impact Analysis: Bewertung der potentiellen Auswirkungen verschiedener KI-Bedrohungen auf Ihre Gesch√§ftsprozesse und strategischen Ziele.\n‚Ä¢ Likelihood Assessment: Einsch√§tzung der Wahrscheinlichkeit verschiedener Angriffszenarien basierend auf Ihrer spezifischen Bedrohungslandschaft.\n‚Ä¢ Risk Appetite Alignment: Anpassung der Sicherheitsma√ünahmen an Ihre Risikotoleranz und Gesch√§ftsstrategie.\n‚Ä¢ Continuous Threat Landscape Monitoring: Regelm√§√üige Aktualisierung der Bedrohungsbewertung basierend auf neuen Entwicklungen und Erkenntnissen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 2),
        question: 'Wie k√∂nnen Adversarial Attacks unsere KI-Systeme kompromittieren und welche Schutzma√ünahmen implementiert ADVISORI dagegen?',
        answer: "Adversarial Attacks stellen eine der sophistiziertesten und gef√§hrlichsten Bedrohungen f√ºr KI-Systeme dar. Diese gezielten Angriffe nutzen die inh√§renten Schw√§chen von Machine Learning-Modellen aus, um durch minimal ver√§nderte Eingaben drastisch falsche Ausgaben zu erzeugen. F√ºr Unternehmen k√∂nnen solche Angriffe katastrophale Folgen haben, von fehlerhaften Gesch√§ftsentscheidungen bis hin zu Sicherheitsverletzungen. ADVISORI entwickelt mehrschichtige Verteidigungsstrategien, die sowohl pr√§ventive als auch reaktive Ma√ünahmen umfassen.\n\n‚öîÔ∏è Adversarial Attack Mechanismen und Gesch√§ftsrisiken:\n‚Ä¢ Evasion Attacks: Manipulation von Eingabedaten zur Laufzeit, um Klassifikationsfehler zu provozieren, beispielsweise bei Betrugserkennungssystemen oder Sicherheitsscannern.\n‚Ä¢ Poisoning Attacks: Einschleusung manipulierter Daten w√§hrend des Trainingsprozesses, um systematische Schwachstellen oder Backdoors zu schaffen.\n‚Ä¢ Model Inversion: Rekonstruktion sensibler Trainingsdaten durch gezielte Abfragen, was zu Datenschutzverletzungen f√ºhren kann.\n‚Ä¢ Membership Inference: Bestimmung, ob spezifische Daten im Trainingssatz enthalten waren, was R√ºckschl√ºsse auf vertrauliche Informationen erm√∂glicht.\n\nüõ°Ô∏è ADVISORI's Multi-Layer Defense Strategy:\n‚Ä¢ Adversarial Training: Implementierung robuster Trainingsverfahren, die Modelle gegen bekannte Angriffsmuster immunisieren.\n‚Ä¢ Input Sanitization und Validation: Entwicklung intelligenter Eingabefilter, die verd√§chtige oder manipulierte Daten vor der Verarbeitung erkennen und neutralisieren.\n‚Ä¢ Ensemble Methods: Einsatz mehrerer diverser Modelle zur Kreuzvalidierung von Entscheidungen und Erkennung von Anomalien.\n‚Ä¢ Gradient Masking und Obfuscation: Verschleierung der Modellarchitektur und -parameter, um Angreifern die Entwicklung gezielter Adversarial Examples zu erschweren.\n\nüî¨ Proaktive Robustness Testing und Validation:\n‚Ä¢ Automated Adversarial Testing: Kontinuierliche Generierung und Testung von Adversarial Examples zur Bewertung der Modellrobustheit.\n‚Ä¢ Certified Defense Mechanisms: Implementierung mathematisch beweisbarer Verteidigungsverfahren mit garantierten Robustheitseigenschaften.\n‚Ä¢ Real-time Anomaly Detection: √úberwachung von Modelleingaben und -ausgaben zur Erkennung verd√§chtiger Muster oder ungew√∂hnlicher Verhaltensweisen.\n‚Ä¢ Continuous Model Monitoring: Langfristige √úberwachung der Modellleistung zur Fr√ºherkennung von Performance-Degradation oder Kompromittierung.\n\nüìä Business Continuity und Incident Response:\n‚Ä¢ Graceful Degradation Strategies: Entwicklung von Fallback-Mechanismen, die bei erkannten Angriffen sichere Standardverhalten aktivieren.\n‚Ä¢ Rapid Response Protocols: Etablierung schneller Reaktionsverfahren zur Isolation kompromittierter Systeme und Wiederherstellung des sicheren Betriebs.\n‚Ä¢ Forensic Capabilities: Implementierung umfassender Logging- und Audit-Funktionen zur Nachverfolgung und Analyse von Sicherheitsvorf√§llen.\n‚Ä¢ Stakeholder Communication: Vorbereitung transparenter Kommunikationsstrategien f√ºr den Fall von Sicherheitsvorf√§llen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 3),
        question: 'Welche Rolle spielt Data Poisoning bei KI-Angriffen und wie sch√ºtzt ADVISORI die Integrit√§t unserer Trainingsdaten?',
        answer: "Data Poisoning stellt eine besonders heimt√ºckische Bedrohung dar, da es die Grundlage jedes KI-Systems - die Trainingsdaten - kompromittiert. Im Gegensatz zu anderen Angriffsformen, die zur Laufzeit stattfinden, erfolgt Data Poisoning bereits w√§hrend der Modellentwicklung und kann daher schwer zu erkennen sein. Die Auswirkungen k√∂nnen verheerend sein, da kompromittierte Modelle systematisch falsche Entscheidungen treffen oder versteckte Backdoors enthalten k√∂nnen. ADVISORI implementiert umfassende Datenintegrit√§ts- und Validierungsframeworks, die diese Bedrohung von der Datensammlung bis zum Modelldeployment adressieren.\n\nüß¨ Data Poisoning Angriffsvektoren und Gesch√§ftsauswirkungen:\n‚Ä¢ Label Flipping: Systematische Manipulation von Datenklassifikationen, die zu grundlegend fehlerhaften Modellentscheidungen f√ºhren k√∂nnen.\n‚Ä¢ Feature Poisoning: Subtile Ver√§nderungen in Eingabemerkmalen, die Modelle f√ºr spezifische Trigger-Patterns anf√§llig machen.\n‚Ä¢ Backdoor Injection: Einbettung versteckter Trigger in Trainingsdaten, die sp√§ter zur Aktivierung unerw√ºnschter Modellverhalten genutzt werden k√∂nnen.\n‚Ä¢ Distribution Shift Attacks: Gezielte Verzerrung der Datenverteilung, um Modellleistung in kritischen Bereichen zu degradieren.\n\nüîç ADVISORI's Comprehensive Data Integrity Framework:\n‚Ä¢ Multi-Source Data Validation: Implementierung redundanter Datenquellen und Kreuzvalidierung zur Erkennung von Inkonsistenzen oder Manipulationen.\n‚Ä¢ Statistical Anomaly Detection: Einsatz fortschrittlicher statistischer Methoden zur Identifikation ungew√∂hnlicher Muster oder Ausrei√üer in Trainingsdaten.\n‚Ä¢ Provenance Tracking: Vollst√§ndige Nachverfolgung der Datenherkunft und -verarbeitung zur Sicherstellung der Datenintegrit√§t entlang der gesamten Pipeline.\n‚Ä¢ Automated Data Quality Assessment: Kontinuierliche Bewertung der Datenqualit√§t durch automatisierte Metriken und Qualit√§tsindikatoren.\n\nüõ°Ô∏è Proaktive Schutzma√ünahmen und Robust Training:\n‚Ä¢ Differential Privacy: Implementierung von Datenschutztechniken, die die Auswirkungen einzelner manipulierter Datenpunkte begrenzen.\n‚Ä¢ Robust Aggregation Methods: Einsatz von Trainingsverfahren, die gegen√ºber einer begrenzten Anzahl kompromittierter Datenpunkte resilient sind.\n‚Ä¢ Data Sanitization Pipelines: Entwicklung automatisierter Bereinigungsverfahren zur Entfernung verd√§chtiger oder inkonsistenter Daten.\n‚Ä¢ Federated Learning Security: Implementierung sicherer verteilter Lernverfahren, die lokale Datenmanipulation erkennen und neutralisieren k√∂nnen.\n\nüìà Continuous Monitoring und Adaptive Defense:\n‚Ä¢ Model Performance Monitoring: Kontinuierliche √úberwachung der Modellleistung zur Fr√ºherkennung von Performance-Degradation durch Data Poisoning.\n‚Ä¢ Drift Detection: Implementierung von Verfahren zur Erkennung unerwarteter √Ñnderungen in Datenverteilungen oder Modellverhalten.\n‚Ä¢ Incremental Learning Security: Sichere Verfahren f√ºr die kontinuierliche Modellaktualisierung ohne Risiko der Kontamination durch neue Daten.\n‚Ä¢ Threat Intelligence Integration: Einbindung aktueller Threat Intelligence zur Anpassung der Schutzma√ünahmen an neue Angriffsmethoden."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 4),
        question: 'Wie gew√§hrleistet ADVISORI DSGVO-Compliance bei gleichzeitiger Implementierung effektiver KI-Sicherheitsma√ünahmen?',
        answer: "Die Herausforderung, KI-Sicherheit und DSGVO-Compliance zu vereinen, erfordert einen integrierten Ansatz, der Datenschutz nicht als Hindernis, sondern als fundamentalen Baustein sicherer KI-Systeme betrachtet. ADVISORI entwickelt Privacy-by-Design-Architekturen, die sowohl h√∂chste Sicherheitsstandards als auch vollst√§ndige DSGVO-Konformit√§t gew√§hrleisten. Unser Ansatz zeigt, dass Datenschutz und Sicherheit sich gegenseitig verst√§rken k√∂nnen, anstatt in Konflikt zu stehen.\n\nüîí Privacy-by-Design f√ºr KI-Sicherheit:\n‚Ä¢ Differential Privacy Implementation: Einsatz mathematisch beweisbarer Datenschutztechniken, die gleichzeitig vor Membership Inference Attacks und anderen Privacy-Verletzungen sch√ºtzen.\n‚Ä¢ Federated Learning Architectures: Implementierung verteilter Lernverfahren, die Daten lokal belassen und dennoch robuste, sichere Modelle erm√∂glichen.\n‚Ä¢ Homomorphic Encryption: Nutzung verschl√ºsselter Berechnungen f√ºr KI-Inferenz, die sowohl Datenschutz als auch Schutz vor Datenextraktion gew√§hrleisten.\n‚Ä¢ Secure Multi-Party Computation: Erm√∂glichung kollaborativer KI-Entwicklung ohne Preisgabe sensibler Daten zwischen Parteien.\n\n‚öñÔ∏è DSGVO-konforme Sicherheitsgovernance:\n‚Ä¢ Data Minimization Strategies: Implementierung von Verfahren, die nur die minimal notwendigen Daten f√ºr KI-Training und -Betrieb verwenden.\n‚Ä¢ Purpose Limitation Enforcement: Technische Ma√ünahmen zur Sicherstellung, dass KI-Systeme nur f√ºr ihre deklarierten Zwecke verwendet werden k√∂nnen.\n‚Ä¢ Consent Management Integration: Entwicklung von Systemen, die Einwilligungen granular verwalten und bei Widerruf entsprechende Sicherheitsma√ünahmen aktivieren.\n‚Ä¢ Right to Explanation Implementation: Bereitstellung erkl√§rbarer KI-Entscheidungen, die gleichzeitig Transparenz und Schutz vor Model Extraction bieten.\n\nüõ°Ô∏è Integrierte Sicherheits- und Datenschutzarchitekturen:\n‚Ä¢ Privacy-Preserving Anomaly Detection: Entwicklung von Sicherheits√ºberwachungssystemen, die Bedrohungen erkennen, ohne personenbezogene Daten zu kompromittieren.\n‚Ä¢ Pseudonymization und Anonymization: Implementierung fortschrittlicher Anonymisierungstechniken, die sowohl DSGVO-Anforderungen erf√ºllen als auch Sicherheitsanalysen erm√∂glichen.\n‚Ä¢ Secure Data Deletion: Entwicklung von Verfahren f√ºr das sichere und nachweisbare L√∂schen von Daten aus KI-Systemen bei Aus√ºbung des Rechts auf Vergessenwerden.\n‚Ä¢ Cross-Border Data Protection: Implementierung von Sicherheitsma√ünahmen, die internationale Datentransfers DSGVO-konform absichern.\n\nüìä Compliance Monitoring und Audit Readiness:\n‚Ä¢ Automated Compliance Checking: Kontinuierliche √úberwachung der DSGVO-Konformit√§t durch automatisierte Systeme, die gleichzeitig Sicherheitsverletzungen erkennen.\n‚Ä¢ Comprehensive Audit Trails: Implementierung l√ºckenloser Protokollierung, die sowohl Sicherheits- als auch Datenschutz-Audits unterst√ºtzt.\n‚Ä¢ Impact Assessment Integration: Entwicklung von Verfahren, die Datenschutz-Folgenabsch√§tzungen mit Sicherheitsrisikoanalysen kombinieren.\n‚Ä¢ Incident Response Coordination: Etablierung von Prozessen, die sowohl Sicherheitsvorf√§lle als auch Datenschutzverletzungen koordiniert behandeln."
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQs batch 1 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
