import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating IAM Architektur page with FAQ batch 3...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'iam-architektur' })
    
    if (!existingDoc) {
      throw new Error('Document "iam-architektur" not found')
    }
    
    // Create new FAQs for Security frameworks and Zero-Trust architectural integration
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 9),
        question: 'Wie integriert man Zero-Trust-Prinzipien erfolgreich in eine IAM-Architektur und welche architektonischen √Ñnderungen sind erforderlich, um kontinuierliche Verifikation zu gew√§hrleisten?',
        answer: "Die Integration von Zero-Trust-Prinzipien in eine IAM-Architektur erfordert fundamentale architektonische Ver√§nderungen, die √ºber traditionelle perimeter-basierte Sicherheitsmodelle hinausgehen. Zero-Trust transformiert IAM von einem statischen Gatekeeper zu einem dynamischen, intelligenten Sicherheitsorchestrator, der kontinuierliche Verifikation und adaptive Autorisierung erm√∂glicht.\n\nüõ°Ô∏è Zero-Trust Architecture Fundamentals:\n‚Ä¢ Never Trust, Always Verify als Grundprinzip f√ºr alle Identit√§ts- und Zugriffsentscheidungen\n‚Ä¢ Continuous Authentication mit Multi-Factor und Behavioral Verification\n‚Ä¢ Least Privilege Access mit Just-in-Time und Just-Enough-Access Principles\n‚Ä¢ Micro-Segmentation f√ºr granulare Netzwerk- und Anwendungssicherheit\n‚Ä¢ Assume Breach Mentality mit kontinuierlicher √úberwachung und Anomaly Detection\n\nüîç Continuous Verification Architecture:\n‚Ä¢ Real-time Risk Assessment mit Machine Learning und Behavioral Analytics\n‚Ä¢ Context-aware Authentication basierend auf Benutzer, Ger√§t, Standort und Verhalten\n‚Ä¢ Dynamic Policy Enforcement mit adaptiven Sicherheitsrichtlinien\n‚Ä¢ Session Monitoring mit kontinuierlicher Vertrauensbewertung\n‚Ä¢ Adaptive Multi-Factor Authentication basierend auf Risikobewertung\n\nüèóÔ∏è Architectural Components und Patterns:\n‚Ä¢ Policy Decision Point f√ºr centralized Authorization Decisions\n‚Ä¢ Policy Enforcement Points an allen kritischen Zugriffspunkten\n‚Ä¢ Identity Verification Service mit Multi-Source Identity Validation\n‚Ä¢ Risk Engine f√ºr Real-time Threat Assessment und Scoring\n‚Ä¢ Audit und Analytics Engine f√ºr Compliance und Forensics\n\nüåê Network und Application Integration:\n‚Ä¢ Software-Defined Perimeter f√ºr sichere Netzwerksegmentierung\n‚Ä¢ Service Mesh Integration f√ºr Service-to-Service Zero-Trust\n‚Ä¢ API Gateway mit Zero-Trust Policy Enforcement\n‚Ä¢ Cloud Access Security Broker f√ºr Cloud-Service-Protection\n‚Ä¢ Endpoint Detection und Response f√ºr Device Trust Verification\n\nüìä Data-Centric Security Architecture:\n‚Ä¢ Data Classification und Labeling f√ºr granulare Zugriffskontrollen\n‚Ä¢ Encryption-at-Rest und Encryption-in-Transit f√ºr umfassenden Datenschutz\n‚Ä¢ Data Loss Prevention mit Real-time Monitoring und Blocking\n‚Ä¢ Rights Management f√ºr dokumentenbasierte Zugriffskontrollen\n‚Ä¢ Privacy-preserving Analytics f√ºr GDPR-konforme Datennutzung\n\nüîÑ Implementation Strategy und Migration:\n‚Ä¢ Phased Rollout mit Pilot-Implementierung und graduelle Expansion\n‚Ä¢ Legacy System Integration mit Adapter-Pattern und Proxy-Services\n‚Ä¢ User Experience Optimization f√ºr nahtlose Zero-Trust-Adoption\n‚Ä¢ Performance Optimization f√ºr minimale Latency-Impact\n‚Ä¢ Change Management f√ºr organisatorische Zero-Trust-Transformation"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 10),
        question: 'Welche Rolle spielt Data Architecture in modernen IAM-Systemen und wie konzipiert man skalierbare Datenmodelle, die sowohl Performance als auch Compliance-Anforderungen optimal erf√ºllen?',
        answer: "Data Architecture bildet das Fundament moderner IAM-Systeme und bestimmt ma√ügeblich Performance, Skalierbarkeit und Compliance-F√§higkeiten. Eine durchdachte Datenarchitektur muss komplexe Identit√§tsbeziehungen modellieren, w√§hrend sie gleichzeitig regulatorische Anforderungen erf√ºllt und extreme Skalierung erm√∂glicht.\n\nüìä Identity Data Modeling Strategies:\n‚Ä¢ Graph Database Integration f√ºr komplexe Identit√§tsbeziehungen und Hierarchien\n‚Ä¢ Polyglot Persistence mit verschiedenen Datenbanktechnologien f√ºr optimale Performance\n‚Ä¢ Event Sourcing f√ºr vollst√§ndige Audit-Trails und State Reconstruction\n‚Ä¢ CQRS Implementation f√ºr optimierte Read/Write-Performance\n‚Ä¢ Data Partitioning und Sharding f√ºr horizontale Skalierung\n\nüîÑ Real-time Data Processing Architecture:\n‚Ä¢ Stream Processing f√ºr Event-driven Identity Updates\n‚Ä¢ Change Data Capture f√ºr Real-time Synchronization\n‚Ä¢ Event-driven Architecture f√ºr lose gekoppelte Datenverarbeitung\n‚Ä¢ Message Queues f√ºr asynchrone Datenverarbeitung\n‚Ä¢ Complex Event Processing f√ºr Pattern Recognition und Analytics\n\nüõ°Ô∏è Data Security und Privacy-by-Design:\n‚Ä¢ Encryption-at-Rest mit Advanced Key Management\n‚Ä¢ Field-level Encryption f√ºr sensitive Identity Attributes\n‚Ä¢ Data Masking und Tokenization f√ºr Privacy Protection\n‚Ä¢ Right to be Forgotten Implementation f√ºr GDPR Compliance\n‚Ä¢ Data Lineage Tracking f√ºr Compliance und Audit\n\nüìà Scalable Data Architecture Patterns:\n‚Ä¢ Database Sharding Strategies f√ºr Multi-Tenant Environments\n‚Ä¢ Read Replicas und Write-through Caching f√ºr Performance\n‚Ä¢ Data Lake Integration f√ºr Analytics und Machine Learning\n‚Ä¢ Time-series Databases f√ºr Audit und Behavioral Data\n‚Ä¢ Distributed Caching f√ºr Low-latency Data Access\n\nüîç Analytics und Intelligence Architecture:\n‚Ä¢ Data Warehouse Integration f√ºr Business Intelligence\n‚Ä¢ Real-time Analytics f√ºr Security Monitoring\n‚Ä¢ Machine Learning Pipeline f√ºr Behavioral Analysis\n‚Ä¢ Predictive Analytics f√ºr Risk Assessment\n‚Ä¢ Identity Analytics f√ºr User Behavior Insights\n\n‚öñÔ∏è Compliance und Governance Architecture:\n‚Ä¢ Data Classification und Labeling f√ºr Regulatory Compliance\n‚Ä¢ Retention Policies mit automatischer Data Lifecycle Management\n‚Ä¢ Audit Trail Architecture f√ºr Compliance Reporting\n‚Ä¢ Data Quality Management f√ºr Accurate Identity Information\n‚Ä¢ Cross-border Data Transfer Compliance f√ºr Global Operations"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 11),
        question: 'Wie entwickelt man eine resiliente IAM-Architektur, die High Availability gew√§hrleistet und gleichzeitig Disaster Recovery und Business Continuity f√ºr kritische Identit√§tsdienste sicherstellt?',
        answer: "Eine resiliente IAM-Architektur ist essentiell f√ºr Business Continuity und muss sowohl geplante als auch ungeplante Ausf√§lle bew√§ltigen k√∂nnen, ohne kritische Gesch√§ftsprozesse zu beeintr√§chtigen. Resilienz erfordert durchdachte Redundanz, intelligente Failover-Mechanismen und robuste Recovery-Strategien auf allen Architekturebenen.\n\nüèóÔ∏è High Availability Architecture Design:\n‚Ä¢ Multi-Zone Deployment f√ºr geografische Redundanz\n‚Ä¢ Load Balancing mit Health Checks und Automatic Failover\n‚Ä¢ Database Clustering mit Master-Slave und Master-Master Configurations\n‚Ä¢ Stateless Service Design f√ºr horizontale Skalierung und Failover\n‚Ä¢ Circuit Breaker Pattern f√ºr Graceful Degradation\n\nüîÑ Disaster Recovery Architecture:\n‚Ä¢ Hot-Standby Systems f√ºr minimale Recovery Time Objectives\n‚Ä¢ Cross-Region Replication f√ºr geografische Disaster Protection\n‚Ä¢ Backup und Recovery Automation mit Point-in-Time Recovery\n‚Ä¢ Recovery Testing und Validation f√ºr Disaster Preparedness\n‚Ä¢ Business Impact Analysis f√ºr Priority-based Recovery Planning\n\nüìä Data Resilience und Consistency:\n‚Ä¢ Database Replication Strategies f√ºr Data Availability\n‚Ä¢ Eventual Consistency Patterns f√ºr Distributed Systems\n‚Ä¢ Conflict Resolution Mechanisms f√ºr Multi-Master Scenarios\n‚Ä¢ Data Integrity Checks und Corruption Detection\n‚Ä¢ Backup Verification und Recovery Testing\n\nüåê Network Resilience und Connectivity:\n‚Ä¢ Multi-Path Networking f√ºr Redundant Connectivity\n‚Ä¢ DNS Failover f√ºr Automatic Service Redirection\n‚Ä¢ Content Delivery Networks f√ºr Global Availability\n‚Ä¢ Network Segmentation f√ºr Fault Isolation\n‚Ä¢ Bandwidth Management f√ºr Performance under Load\n\nüîß Operational Resilience Patterns:\n‚Ä¢ Health Monitoring mit Proactive Alerting\n‚Ä¢ Automated Recovery Procedures f√ºr Common Failure Scenarios\n‚Ä¢ Capacity Planning f√ºr Peak Load Management\n‚Ä¢ Performance Degradation Detection und Response\n‚Ä¢ Incident Response Automation f√ºr Faster Recovery\n\nüìã Business Continuity Planning:\n‚Ä¢ Recovery Time Objective und Recovery Point Objective Definition\n‚Ä¢ Business Process Mapping f√ºr Critical Service Identification\n‚Ä¢ Stakeholder Communication Plans f√ºr Incident Management\n‚Ä¢ Regular Disaster Recovery Drills und Testing\n‚Ä¢ Vendor Management f√ºr Third-party Service Continuity"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 12),
        question: 'Welche architektonischen √úberlegungen sind bei der Integration von Artificial Intelligence und Machine Learning in IAM-Systeme erforderlich und wie baut man intelligente, selbstlernende Identit√§tsarchitekturen?',
        answer: "Die Integration von AI und ML in IAM-Architekturen transformiert statische Sicherheitssysteme in intelligente, adaptive Plattformen, die kontinuierlich lernen und sich an neue Bedrohungen anpassen. Diese Integration erfordert spezialisierte architektonische Patterns, die Datenverarbeitung, Modell-Training und Real-time Inference nahtlos vereinen.\n\nü§ñ AI-powered IAM Architecture Components:\n‚Ä¢ Machine Learning Pipeline f√ºr Behavioral Analytics und Anomaly Detection\n‚Ä¢ Real-time Inference Engine f√ºr Dynamic Risk Assessment\n‚Ä¢ Model Training Infrastructure f√ºr Continuous Learning\n‚Ä¢ Feature Engineering Pipeline f√ºr Identity Data Processing\n‚Ä¢ A/B Testing Framework f√ºr Model Performance Validation\n\nüìä Data Architecture f√ºr Machine Learning:\n‚Ä¢ Data Lake Integration f√ºr Historical Identity Data\n‚Ä¢ Feature Store f√ºr Reusable ML Features\n‚Ä¢ Real-time Streaming f√ºr Live Model Inference\n‚Ä¢ Data Labeling und Annotation f√ºr Supervised Learning\n‚Ä¢ Privacy-preserving ML f√ºr Sensitive Identity Data\n\nüß† Intelligent Decision Making Architecture:\n‚Ä¢ Risk Scoring Engine mit Multi-dimensional Analysis\n‚Ä¢ Adaptive Authentication mit ML-based Challenge Selection\n‚Ä¢ Behavioral Biometrics f√ºr Continuous User Verification\n‚Ä¢ Fraud Detection mit Ensemble Learning Methods\n‚Ä¢ Predictive Analytics f√ºr Proactive Security Measures\n\nüîÑ Self-Learning und Adaptive Systems:\n‚Ä¢ Online Learning f√ºr Real-time Model Updates\n‚Ä¢ Feedback Loops f√ºr Continuous Model Improvement\n‚Ä¢ Automated Model Retraining mit Performance Monitoring\n‚Ä¢ Concept Drift Detection f√ºr Model Relevance\n‚Ä¢ Explainable AI f√ºr Transparent Decision Making\n\nüõ°Ô∏è AI Security und Model Protection:\n‚Ä¢ Adversarial Attack Protection f√ºr ML Models\n‚Ä¢ Model Versioning und Rollback Capabilities\n‚Ä¢ Bias Detection und Fairness Monitoring\n‚Ä¢ Model Interpretability f√ºr Compliance Requirements\n‚Ä¢ Secure Model Deployment mit Encrypted Inference\n\n‚ö° Performance und Scalability Considerations:\n‚Ä¢ Edge Computing f√ºr Low-latency ML Inference\n‚Ä¢ Model Optimization f√ºr Resource-efficient Deployment\n‚Ä¢ Distributed Training f√ºr Large-scale Model Development\n‚Ä¢ Caching Strategies f√ºr Frequent ML Predictions\n‚Ä¢ Auto-scaling f√ºr Variable ML Workloads"
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQ batch 3 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
