import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating DORA Informationsregister page with FAQ batch 1...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'dora-informationsregister' })
    
    if (!existingDoc) {
      throw new Error('Document "dora-informationsregister" not found')
    }
    
    // Create new FAQs for DORA information register fundamentals
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 1),
        question: 'Welche spezifischen Informationen m√ºssen in einem DORA-konformen Informationsregister erfasst werden?',
        answer: "DORA verlangt die systematische Erfassung umfassender Informationen √ºber alle kritischen IKT-Assets und -Services, die weit √ºber traditionelle IT-Inventare hinausgehen. Ein DORA-konformes Informationsregister bildet das Fundament f√ºr effektives Risikomanagement und regulatorische Compliance und erfordert strukturierte Dokumentation aller relevanten technischen, operationellen und gesch√§ftlichen Aspekte der IKT-Landschaft.\n\nüèóÔ∏è IKT-Asset-Grunddaten und technische Spezifikationen:\n‚Ä¢ Vollst√§ndige Inventarisierung aller IKT-Systeme, Anwendungen, Datenbanken und Infrastrukturkomponenten mit eindeutigen Identifikatoren\n‚Ä¢ Technische Spezifikationen einschlie√ülich Hardware-Konfigurationen, Software-Versionen, Betriebssysteme und Patch-Level\n‚Ä¢ Netzwerk-Topologie und Interconnection-Details zwischen verschiedenen Systemkomponenten\n‚Ä¢ Kapazit√§ts- und Performance-Parameter sowie aktuelle Auslastungsgrade\n‚Ä¢ Sicherheitskonfigurationen, Verschl√ºsselungsstandards und Authentifizierungsmechanismen\n\nüìä Gesch√§ftskritikalit√§t und Impact-Bewertung:\n‚Ä¢ Klassifizierung der Gesch√§ftskritikalit√§t basierend auf operationellen Auswirkungen bei Systemausf√§llen\n‚Ä¢ Detaillierte Business-Impact-Analysen mit quantifizierten finanziellen und operationellen Konsequenzen\n‚Ä¢ Recovery-Time-Objectives und Recovery-Point-Objectives f√ºr jedes kritische System\n‚Ä¢ Abh√§ngigkeitsmatrizen zwischen verschiedenen IKT-Services und Gesch√§ftsprozessen\n‚Ä¢ Identifikation von Single Points of Failure und kritischen Pfaden in der IKT-Architektur\n\nüîó Drittanbieter-Services und externe Abh√§ngigkeiten:\n‚Ä¢ Vollst√§ndige Dokumentation aller IKT-Drittanbieter mit Kontaktdaten, Vertragsdetails und Service-Level-Agreements\n‚Ä¢ Risikobewertungen f√ºr jeden Drittanbieter einschlie√ülich finanzieller Stabilit√§t und operationeller Zuverl√§ssigkeit\n‚Ä¢ Dokumentation von Sub-Contractors und deren Rolle in der IKT-Service-Bereitstellung\n‚Ä¢ Geografische Verteilung von Drittanbieter-Services und damit verbundene Jurisdiktionsrisiken\n‚Ä¢ Exit-Strategien und Alternative-Provider-Optionen f√ºr kritische Services\n\nüõ°Ô∏è Sicherheits- und Compliance-Informationen:\n‚Ä¢ Aktuelle Schwachstellen-Assessments und Penetration-Test-Ergebnisse f√ºr alle kritischen Systeme\n‚Ä¢ Compliance-Status bez√ºglich relevanter Standards wie ISO 27001, SOC 2 oder branchenspezifischer Anforderungen\n‚Ä¢ Incident-Historie mit Details zu vergangenen Sicherheitsvorf√§llen und deren Aufl√∂sung\n‚Ä¢ Backup- und Disaster-Recovery-Konfigurationen mit regelm√§√üigen Test-Ergebnissen\n‚Ä¢ Zugriffs- und Berechtigungsmatrizen f√ºr alle kritischen Systeme und Daten\n\nüìã Governance- und Verantwortlichkeits-Strukturen:\n‚Ä¢ Klare Zuordnung von System-Ownership und Verantwortlichkeiten auf Personen- und Organisationsebene\n‚Ä¢ Eskalationspfade und Kontaktinformationen f√ºr verschiedene Incident-Szenarien\n‚Ä¢ Change-Management-Prozesse und Genehmigungsworkflows f√ºr System-Modifikationen\n‚Ä¢ Dokumentation von Service-Level-Agreements und operationellen Metriken\n‚Ä¢ Integration mit bestehenden ITSM-Prozessen und Governance-Frameworks"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 2),
        question: 'Wie implementiere ich eine automatisierte Datenerfassung f√ºr mein DORA-Informationsregister?',
        answer: "Die Automatisierung der Datenerfassung ist entscheidend f√ºr die Aufrechterhaltung eines aktuellen und genauen DORA-Informationsregisters. Manuelle Prozesse sind fehleranf√§llig und skalieren nicht mit der Komplexit√§t moderner IKT-Landschaften. Eine effektive Automatisierungsstrategie kombiniert verschiedene Technologien und Ans√§tze, um kontinuierliche Datenqualit√§t und Compliance-Readiness zu gew√§hrleisten.\n\nüîç Asset-Discovery und automatische Inventarisierung:\n‚Ä¢ Implementierung von Network-Discovery-Tools zur automatischen Erkennung aller verbundenen Ger√§te und Services\n‚Ä¢ Integration mit bestehenden Configuration Management Databases f√ºr kontinuierliche Asset-Synchronisation\n‚Ä¢ Verwendung von Agent-basierten Monitoring-L√∂sungen f√ºr detaillierte System-Informationen und Real-Time-Updates\n‚Ä¢ API-Integration mit Cloud-Providern f√ºr automatische Erfassung von Cloud-Ressourcen und deren Konfigurationen\n‚Ä¢ Vulnerability-Scanner-Integration f√ºr kontinuierliche Sicherheitsbewertungen und Patch-Status-Updates\n\n‚öôÔ∏è Datenintegration und Workflow-Automatisierung:\n‚Ä¢ Entwicklung von ETL-Prozessen zur Konsolidierung von Daten aus verschiedenen Quellsystemen\n‚Ä¢ Implementierung von Event-driven Architectures f√ºr Real-Time-Updates bei System-√Ñnderungen\n‚Ä¢ Workflow-Engine-Integration f√ºr automatisierte Genehmigungsprozesse bei kritischen √Ñnderungen\n‚Ä¢ Machine-Learning-basierte Anomalie-Erkennung zur Identifikation ungew√∂hnlicher Konfigurations√§nderungen\n‚Ä¢ Robotic Process Automation f√ºr die Automatisierung repetitiver Datensammlung- und Validierungsaufgaben\n\nüìä Datenqualit√§t und Validierung:\n‚Ä¢ Implementierung von Data-Quality-Rules und automatischen Konsistenzpr√ºfungen\n‚Ä¢ Duplicate-Detection-Algorithmen zur Vermeidung redundanter Eintr√§ge\n‚Ä¢ Automated-Testing-Frameworks f√ºr regelm√§√üige Validierung der Datenintegrit√§t\n‚Ä¢ Exception-Handling und Alert-Mechanismen bei Datenqualit√§tsproblemen\n‚Ä¢ Historische Datenanalyse zur Identifikation von Trends und Mustern in der IKT-Landschaft\n\nüîÑ Change-Management und Lifecycle-Tracking:\n‚Ä¢ Automatische Erkennung und Dokumentation von System-√Ñnderungen durch Integration mit Change-Management-Tools\n‚Ä¢ Lifecycle-Management f√ºr IKT-Assets mit automatischen Alerts bei End-of-Life oder End-of-Support\n‚Ä¢ Version-Control-Integration f√ºr Software-Assets und Konfigurationsdateien\n‚Ä¢ Automated-Compliance-Checking gegen definierte Standards und Policies\n‚Ä¢ Predictive-Analytics f√ºr proaktive Identifikation potenzieller Risiken und Wartungsbedarfe\n\nüõ†Ô∏è Tool-Integration und Plattform-Architektur:\n‚Ä¢ Master-Data-Management-Plattformen f√ºr zentrale Datenverwaltung und Governance\n‚Ä¢ API-First-Ans√§tze f√ºr nahtlose Integration mit bestehenden Enterprise-Systemen\n‚Ä¢ Cloud-native Architekturen f√ºr Skalierbarkeit und Flexibilit√§t\n‚Ä¢ Microservices-basierte Datensammlung f√ºr modulare und wartbare L√∂sungen\n‚Ä¢ Real-Time-Dashboards und Reporting-Engines f√ºr kontinuierliche √úberwachung der Datenqualit√§t"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 3),
        question: 'Welche Rolle spielt das Informationsregister bei der DORA-Incident-Response und wie kann es die Reaktionszeiten verbessern?',
        answer: "Das DORA-Informationsregister ist ein kritischer Enabler f√ºr effektive Incident-Response und kann die Reaktionszeiten erheblich verk√ºrzen, indem es sofortigen Zugang zu allen relevanten Informationen √ºber betroffene Systeme und deren Abh√§ngigkeiten bietet. In Krisensituationen ist Zeit der entscheidende Faktor, und ein gut strukturiertes Informationsregister kann den Unterschied zwischen einer schnellen Wiederherstellung und einem langwierigen Ausfall bedeuten.\n\n‚ö° Sofortige Situationsbewertung und Impact-Analyse:\n‚Ä¢ Real-Time-Zugriff auf kritische System-Informationen erm√∂glicht schnelle Bewertung der Ausfallschwere\n‚Ä¢ Automatische Impact-Berechnung basierend auf vordefinierten Business-Criticality-Ratings und Abh√§ngigkeitsmatrizen\n‚Ä¢ Sofortige Identifikation aller betroffenen nachgelagerten Services und Gesch√§ftsprozesse\n‚Ä¢ Geografische und organisatorische Auswirkungsanalyse f√ºr koordinierte Response-Ma√ünahmen\n‚Ä¢ Historische Incident-Daten f√ºr Pattern-Recognition und Lessons-Learned-Integration\n\nüéØ Pr√§zise Eskalation und Ressourcen-Mobilisierung:\n‚Ä¢ Automatische Identifikation der richtigen Ansprechpartner basierend auf System-Ownership und Expertise-Bereichen\n‚Ä¢ Vordefinierte Eskalationsmatrizen mit Kontaktdaten und Verf√ºgbarkeitsinformationen\n‚Ä¢ Skill-basierte Routing von Incidents an die am besten qualifizierten Response-Teams\n‚Ä¢ Integration mit On-Call-Systemen f√ºr automatische Benachrichtigung relevanter Experten\n‚Ä¢ Vendor-Kontaktinformationen und Support-Level-Details f√ºr externe Unterst√ºtzung\n\nüîß Beschleunigte Diagnose und Troubleshooting:\n‚Ä¢ Sofortiger Zugriff auf System-Konfigurationen, Abh√§ngigkeiten und bekannte Schwachstellen\n‚Ä¢ Historische Performance-Daten und Baseline-Metriken f√ºr Anomalie-Identifikation\n‚Ä¢ Dokumentierte Troubleshooting-Procedures und bew√§hrte L√∂sungsans√§tze f√ºr √§hnliche Incidents\n‚Ä¢ Integration mit Monitoring-Tools f√ºr Real-Time-System-Status und Diagnostic-Informationen\n‚Ä¢ Automated-Runbook-Execution basierend auf Incident-Typ und betroffenen Systemen\n\nüõ°Ô∏è Koordinierte Recovery und Business-Continuity:\n‚Ä¢ Sofortiger Zugriff auf Disaster-Recovery-Pl√§ne und Backup-Konfigurationen\n‚Ä¢ Priorisierte Recovery-Sequenzen basierend auf Business-Impact und Abh√§ngigkeiten\n‚Ä¢ Alternative-Service-Provider und Failover-Optionen f√ºr kritische Services\n‚Ä¢ Kommunikationspl√§ne und Stakeholder-Benachrichtigungsmatrizen\n‚Ä¢ Post-Incident-Review-Templates und Lessons-Learned-Dokumentation\n\nüìà Kontinuierliche Verbesserung und Preparedness:\n‚Ä¢ Incident-Response-Metriken und Performance-Tracking f√ºr kontinuierliche Optimierung\n‚Ä¢ Simulation und Tabletop-Exercises basierend auf aktuellen Register-Daten\n‚Ä¢ Proaktive Schwachstellen-Identifikation und Pr√§ventionsma√ünahmen\n‚Ä¢ Integration mit Threat-Intelligence f√ºr kontextuelle Risikobewertung\n‚Ä¢ Automated-Reporting f√ºr regulatorische Anforderungen und Management-Updates"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 4),
        question: 'Wie gew√§hrleiste ich die Datenqualit√§t und Konsistenz in meinem DORA-Informationsregister √ºber verschiedene Datenquellen hinweg?',
        answer: "Die Gew√§hrleistung hoher Datenqualit√§t und Konsistenz in DORA-Informationsregistern ist eine komplexe Herausforderung, die systematische Governance, technische Kontrollen und organisatorische Prozesse erfordert. Inkonsistente oder ungenaue Daten k√∂nnen zu fehlerhaften Risikobewertungen und ineffektiven Incident-Response-Ma√ünahmen f√ºhren, was die Compliance und operative Resilienz gef√§hrdet.\n\nüéØ Master-Data-Management und Datengovernance:\n‚Ä¢ Etablierung einer Single Source of Truth f√ºr alle kritischen IKT-Asset-Informationen\n‚Ä¢ Definition klarer Data-Ownership und Verantwortlichkeiten f√ºr verschiedene Datenkategorien\n‚Ä¢ Implementierung von Data-Stewardship-Rollen mit spezifischen Qualit√§tssicherungsaufgaben\n‚Ä¢ Entwicklung umfassender Data-Dictionaries und Standardisierung von Terminologie\n‚Ä¢ Regelm√§√üige Data-Governance-Reviews und Qualit√§ts-Audits\n\nüîç Automatisierte Datenvalidierung und Qualit√§tskontrolle:\n‚Ä¢ Implementierung von Business-Rules-Engines f√ºr kontinuierliche Datenvalidierung\n‚Ä¢ Automated-Data-Profiling zur Identifikation von Anomalien und Inkonsistenzen\n‚Ä¢ Cross-Reference-Validation zwischen verschiedenen Datenquellen\n‚Ä¢ Statistical-Analysis f√ºr Outlier-Detection und Plausibilit√§tspr√ºfungen\n‚Ä¢ Real-Time-Monitoring von Datenqualit√§ts-KPIs und Alert-Mechanismen\n\n‚öôÔ∏è Datenintegration und Harmonisierung:\n‚Ä¢ ETL-Prozesse mit robusten Datenbereinigung- und Transformationsregeln\n‚Ä¢ API-basierte Integration f√ºr Real-Time-Synchronisation zwischen Systemen\n‚Ä¢ Data-Mapping und Schema-Harmonisierung f√ºr konsistente Datenstrukturen\n‚Ä¢ Conflict-Resolution-Mechanismen f√ºr widerspr√ºchliche Informationen aus verschiedenen Quellen\n‚Ä¢ Version-Control und Change-Tracking f√ºr alle Datenmodifikationen\n\nüìä Kontinuierliche √úberwachung und Verbesserung:\n‚Ä¢ Implementierung von Data-Quality-Dashboards f√ºr kontinuierliche Transparenz\n‚Ä¢ Automated-Reconciliation-Prozesse f√ºr regelm√§√üige Konsistenzpr√ºfungen\n‚Ä¢ Exception-Reporting und Workflow-basierte Fehlerbehandlung\n‚Ä¢ Trend-Analyse zur Identifikation systematischer Datenqualit√§tsprobleme\n‚Ä¢ Feedback-Loops f√ºr kontinuierliche Verbesserung der Datensammlung- und Validierungsprozesse\n\nüõ†Ô∏è Technische Infrastruktur und Tools:\n‚Ä¢ Data-Lineage-Tracking f√ºr vollst√§ndige Nachvollziehbarkeit von Datenfl√ºssen\n‚Ä¢ Automated-Testing-Frameworks f√ºr regelm√§√üige Validierung der Datenintegrit√§t\n‚Ä¢ Machine-Learning-basierte Anomalie-Erkennung f√ºr proaktive Qualit√§tssicherung\n‚Ä¢ Blockchain-basierte Audit-Trails f√ºr unver√§nderliche Dokumentation von Daten√§nderungen\n‚Ä¢ Cloud-native Data-Quality-Plattformen f√ºr Skalierbarkeit und Performance"
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQ batch 1 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
