import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating Erkl√§rbare KI page with Technical Implementation FAQs batch 2 (German)...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'erklaerbare-ki' })
    
    if (!existingDoc) {
      throw new Error('Document "erklaerbare-ki" not found')
    }
    
    // Create new Technical Implementation FAQs in German
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 5),
        question: "Welche konkreten XAI-Techniken und Methoden setzt ADVISORI ein, um komplexe Machine Learning Modelle interpretierbar zu machen, ohne dabei die Modellperformance zu beeintr√§chtigen?",
        answer: "ADVISORI setzt auf einen multi-methodischen Ansatz zur Implementierung von Explainable AI, der modernste Interpretability-Techniken mit performance-optimierten Implementierungen kombiniert. Unser Ziel ist es, maximale Transparenz zu erreichen, ohne die Vorhersagequalit√§t Ihrer KI-Systeme zu kompromittieren. Wir nutzen sowohl model-agnostic als auch model-specific Ans√§tze, um f√ºr jeden Anwendungsfall die optimale Balance zwischen Erkl√§rbarkeit und Performance zu finden.\n\nüî¨ Model-Agnostic Explainability Techniken:\n‚Ä¢ SHAP (SHapley Additive exPlanations): Implementierung von TreeSHAP, KernelSHAP und DeepSHAP f√ºr verschiedene Modelltypen mit optimierten Berechnungsalgorithmen f√ºr Enterprise-Scale-Anwendungen.\n‚Ä¢ LIME (Local Interpretable Model-agnostic Explanations): Adaptive LIME-Implementierungen mit intelligenter Sampling-Strategien f√ºr stabile und konsistente lokale Erkl√§rungen.\n‚Ä¢ Permutation Feature Importance: Robuste Implementierung mit statistischer Signifikanzpr√ºfung und Confidence-Intervallen f√ºr zuverl√§ssige Feature-Ranking.\n‚Ä¢ Counterfactual Explanations: Generierung von What-if-Szenarien und minimalen √Ñnderungsvorschl√§gen f√ºr bessere Entscheidungsunterst√ºtzung.\n\nüß† Model-Specific Interpretability Ans√§tze:\n‚Ä¢ Attention Mechanisms: Visualisierung und Analyse von Attention-Weights in Transformer-Modellen f√ºr nachvollziehbare NLP- und Computer Vision-Anwendungen.\n‚Ä¢ Gradient-based Methods: Implementierung von Integrated Gradients, GradCAM und Layer-wise Relevance Propagation f√ºr Deep Learning Modelle.\n‚Ä¢ Tree-based Interpretability: Native Feature Importance und Partial Dependence Plots f√ºr Random Forest und Gradient Boosting Modelle.\n‚Ä¢ Linear Model Coefficients: Statistische Analyse und Visualisierung von Koeffizienten in linearen und logistischen Regressionsmodellen.\n\n‚ö° Performance-Optimierte Implementierung:\n‚Ä¢ Efficient Computation: Verwendung von approximativen Methoden und Sampling-Techniken f√ºr skalierbare Erkl√§rungen auch bei gro√üen Datens√§tzen.\n‚Ä¢ Caching und Preprocessing: Intelligente Zwischenspeicherung von Erkl√§rungen und Vorverarbeitung f√ºr Echtzeit-Anwendungen.\n‚Ä¢ Parallel Processing: Multi-Threading und GPU-Acceleration f√ºr schnelle Berechnung komplexer Erkl√§rungen.\n‚Ä¢ Adaptive Explanation Depth: Dynamische Anpassung der Erkl√§rungstiefe basierend auf Anwendungskontext und Performance-Anforderungen."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 6),
        question: "Wie gew√§hrleistet ADVISORI die Konsistenz und Zuverl√§ssigkeit von XAI-Erkl√§rungen √ºber verschiedene Modellversionen und Datenverteilungen hinweg, insbesondere bei kontinuierlichem Model Retraining?",
        answer: "Die Konsistenz und Zuverl√§ssigkeit von XAI-Erkl√§rungen ist entscheidend f√ºr das Vertrauen in KI-Systeme, insbesondere in dynamischen Umgebungen mit kontinuierlichem Model Retraining. ADVISORI implementiert robuste Monitoring- und Validierungssysteme, die sicherstellen, dass Erkl√§rungen √ºber Zeit und Modellversionen hinweg stabil und vertrauensw√ºrdig bleiben.\n\nüìä Explanation Consistency Monitoring:\n‚Ä¢ Explanation Drift Detection: Kontinuierliche √úberwachung von Ver√§nderungen in Feature Importance und Erkl√§rungsmustern zwischen Modellversionen mit statistischen Tests und Anomalie-Erkennung.\n‚Ä¢ Stability Metrics: Implementierung von Konsistenz-Metriken wie Explanation Fidelity, Stability Score und Feature Ranking Correlation f√ºr quantitative Bewertung der Erkl√§rungsqualit√§t.\n‚Ä¢ Cross-Version Validation: Systematischer Vergleich von Erkl√§rungen zwischen verschiedenen Modellversionen mit automatisierten Alerts bei signifikanten Abweichungen.\n‚Ä¢ Temporal Consistency Analysis: Analyse von Erkl√§rungsmustern √ºber Zeit hinweg zur Identifikation von Trends und unerwarteten Ver√§nderungen.\n\nüîÑ Robust Explanation Generation:\n‚Ä¢ Ensemble Explanations: Kombination mehrerer Erkl√§rungsmethoden f√ºr robustere und stabilere Insights mit Confidence-Scoring f√ºr jede Erkl√§rung.\n‚Ä¢ Bootstrap Sampling: Verwendung von Bootstrap-Methoden zur Sch√§tzung der Unsicherheit in Erkl√§rungen und Generierung von Confidence-Intervallen.\n‚Ä¢ Adversarial Robustness: Testing der Erkl√§rungen gegen kleine Eingabeperturbationen zur Sicherstellung der Stabilit√§t gegen√ºber Rauschen.\n‚Ä¢ Reference Point Standardization: Verwendung konsistenter Referenzpunkte und Baseline-Werte f√ºr vergleichbare Erkl√§rungen √ºber verschiedene Modellversionen.\n\nüéØ Adaptive Explanation Frameworks:\n‚Ä¢ Context-Aware Explanations: Anpassung der Erkl√§rungstiefe und -art basierend auf Datenverteilung und Modellkomplexit√§t f√ºr optimale Relevanz.\n‚Ä¢ Dynamic Threshold Management: Automatische Anpassung von Erkl√§rungsschwellenwerten basierend auf Modellperformance und Datencharakteristika.\n‚Ä¢ Explanation Versioning: Systematische Versionierung und Archivierung von Erkl√§rungsmodellen parallel zu ML-Modellversionen f√ºr Nachvollziehbarkeit.\n‚Ä¢ Continuous Calibration: Regelm√§√üige Kalibrierung von Erkl√§rungsmodellen gegen Ground Truth und Expertenwissen f√ºr anhaltende Genauigkeit."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 7),
        question: "Welche spezifischen Herausforderungen entstehen bei der Implementierung von XAI in hochregulierten Branchen und wie adressiert ADVISORI die besonderen Anforderungen von Finanzdienstleistungen, Healthcare und Automotive?",
        answer: "Hochregulierte Branchen stellen besondere Anforderungen an Explainable AI, die √ºber technische Implementierung hinausgehen und spezifische Compliance-, Sicherheits- und Qualit√§tsstandards erf√ºllen m√ºssen. ADVISORI hat spezialisierte XAI-Frameworks f√ºr verschiedene regulierte Branchen entwickelt, die sowohl technische Excellence als auch regulatorische Compliance gew√§hrleisten.\n\nüè¶ Finanzdienstleistungen - Regulatory Excellence:\n‚Ä¢ MiFID II und GDPR Compliance: Implementierung von Right-to-Explanation-konformen Erkl√§rungssystemen mit audit-f√§higer Dokumentation f√ºr automatisierte Entscheidungen.\n‚Ä¢ Model Risk Management: Integration von XAI in bestehende Model Risk Management Frameworks mit quantitativen Risikometriken und Stress-Testing der Erkl√§rungen.\n‚Ä¢ Fair Lending Compliance: Spezialisierte Bias-Detection und Fairness-Monitoring f√ºr Kreditentscheidungen mit demografischen Parity-Checks und Disparate Impact-Analysen.\n‚Ä¢ Regulatory Reporting: Automatisierte Generierung von regulatorischen Reports mit XAI-basierten Begr√ºndungen f√ºr Aufsichtsbeh√∂rden wie BaFin und EBA.\n\nüè• Healthcare - Patient Safety und Clinical Excellence:\n‚Ä¢ FDA und CE-MDR Compliance: Entwicklung von XAI-Systemen f√ºr Medizinprodukte mit klinischer Validierung und Post-Market-Surveillance-Integration.\n‚Ä¢ Clinical Decision Support: Implementierung von evidenzbasierten Erkl√§rungen, die medizinische Guidelines und Best Practices referenzieren f√ºr bessere Arzt-Akzeptanz.\n‚Ä¢ Patient Privacy Protection: HIPAA-konforme XAI-Implementierungen mit Differential Privacy und Federated Learning f√ºr Datenschutz-preservierende Erkl√§rungen.\n‚Ä¢ Clinical Workflow Integration: Nahtlose Integration von XAI in bestehende Electronic Health Record Systeme mit kontextuellen Erkl√§rungen f√ºr verschiedene Stakeholder.\n\nüöó Automotive - Safety-Critical AI Systems:\n‚Ä¢ ISO 26262 Functional Safety: Entwicklung von XAI-Systemen f√ºr safety-critical Automotive-Anwendungen mit ASIL-konformer Dokumentation und Hazard Analysis.\n‚Ä¢ UNECE WP.29 Compliance: Implementierung von XAI f√ºr autonome Fahrsysteme entsprechend internationaler Regulierungsstandards f√ºr automatisierte Fahrzeuge.\n‚Ä¢ Real-time Explanation Generation: Hochperformante XAI-Systeme f√ºr Echtzeit-Entscheidungen in autonomen Fahrzeugen mit Latenz-optimierten Erkl√§rungsalgorithmen.\n‚Ä¢ Incident Investigation Support: Forensische XAI-Capabilities f√ºr Post-Incident-Analysen mit detaillierter Rekonstruktion von Entscheidungspfaden."
      },
      {
        _type: 'object',
        _key: generateKey('faq', 8),
        question: "Wie entwickelt ADVISORI stakeholder-spezifische Erkl√§rungsmodelle, die sowohl f√ºr technische Teams als auch f√ºr Endnutzer und Regulierungsbeh√∂rden verst√§ndlich und actionable sind?",
        answer: "Die Entwicklung stakeholder-spezifischer Erkl√§rungsmodelle ist ein Kernbestandteil von ADVISORI's XAI-Strategie. Wir verstehen, dass verschiedene Zielgruppen unterschiedliche Informationsbed√ºrfnisse, technische Hintergr√ºnde und Entscheidungskontext haben. Unser multi-layered Explanation Framework erm√∂glicht es, aus derselben KI-Entscheidung verschiedene Erkl√§rungsebenen zu generieren, die jeweils optimal auf die spezifischen Bed√ºrfnisse der Zielgruppe zugeschnitten sind.\n\nüë®‚Äçüíª Technical Teams - Deep Dive Explanations:\n‚Ä¢ Feature Engineering Insights: Detaillierte Analyse der Feature-Transformationen und deren Einfluss auf Modellentscheidungen mit Code-Level-Nachvollziehbarkeit.\n‚Ä¢ Model Architecture Explanations: Visualisierung von Modellstrukturen, Attention-Mechanismen und Layer-wise Aktivierungen f√ºr Deep Learning Modelle.\n‚Ä¢ Performance Debugging: Granulare Analyse von Modellfehlern mit Feature-Level-Attribution und Konfidenz-Intervallen f√ºr systematische Modellverbesserung.\n‚Ä¢ Hyperparameter Impact Analysis: Quantifizierung des Einflusses verschiedener Hyperparameter auf Erkl√§rungen und Modellverhalten.\n\nüë• End Users - Intuitive und Actionable Insights:\n‚Ä¢ Natural Language Explanations: Automatische Generierung verst√§ndlicher Textbeschreibungen von KI-Entscheidungen in nat√ºrlicher Sprache ohne technischen Jargon.\n‚Ä¢ Visual Explanation Interfaces: Intuitive Dashboards mit interaktiven Visualisierungen, die komplexe Zusammenh√§nge durch Charts, Heatmaps und What-if-Szenarien erkl√§ren.\n‚Ä¢ Contextual Recommendations: Actionable Empfehlungen basierend auf XAI-Insights, die Nutzern konkrete Handlungsoptionen aufzeigen.\n‚Ä¢ Confidence Communication: Verst√§ndliche Darstellung von Unsicherheit und Konfidenz in KI-Entscheidungen mit Risiko-Kommunikation.\n\n‚öñÔ∏è Regulatory Bodies - Compliance-Ready Documentation:\n‚Ä¢ Audit Trail Generation: Comprehensive Dokumentation aller Entscheidungsschritte mit Zeitstempeln, Datenquellen und verwendeten Algorithmen f√ºr regulatorische Pr√ºfungen.\n‚Ä¢ Statistical Validation Reports: Quantitative Bewertung der Erkl√§rungsqualit√§t mit statistischen Tests, Signifikanz-Analysen und Robustness-Metriken.\n‚Ä¢ Bias and Fairness Assessment: Systematische Analyse von Diskriminierungsrisiken mit demografischen Aufschl√ºsselungen und Fairness-Metriken entsprechend regulatorischer Standards.\n‚Ä¢ Compliance Mapping: Direkte Zuordnung von XAI-Outputs zu spezifischen regulatorischen Anforderungen wie GDPR Article 22 oder EU AI Act Transparency-Verpflichtungen."
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new Technical Implementation FAQs (German) to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ Technical Implementation FAQs batch 2 (German) added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
