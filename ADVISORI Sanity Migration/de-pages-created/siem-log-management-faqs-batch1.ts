import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating SIEM Log Management page with FAQ batch 1...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'siem-log-management' })
    
    if (!existingDoc) {
      throw new Error('Document "siem-log-management" not found')
    }
    
    // Create new FAQs for SIEM Log Management
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 1),
        question: 'Wie entwickelt man eine strategische Log-Architektur f√ºr SIEM-Systeme und welche Faktoren bestimmen die optimale Datensammlung?',
        answer: "Eine strategische Log-Architektur bildet das Fundament f√ºr effektive SIEM-Operations und erfordert eine durchdachte Balance zwischen umfassender Visibility und operativer Effizienz. Die Entwicklung einer optimalen Log-Sammlung-Strategie geht weit √ºber technische Aspekte hinaus und umfasst Business-Alignment, Compliance-Anforderungen und zukunftsorientierte Skalierbarkeit.\n\nüéØ Strategic Log Source Assessment:\n‚Ä¢ Comprehensive Inventory aller verf√ºgbaren Log-Quellen mit Bewertung ihrer Security-Relevanz und Business-Kritikalit√§t\n‚Ä¢ Risk-based Prioritization zur Identifikation der wichtigsten Datenquellen f√ºr Threat Detection und Compliance\n‚Ä¢ Data Quality Assessment zur Bewertung der Vollst√§ndigkeit und Zuverl√§ssigkeit verschiedener Log-Streams\n‚Ä¢ Cost-Benefit Analysis f√ºr jede Log-Quelle unter Ber√ºcksichtigung von Speicher-, Processing- und Analyse-Kosten\n‚Ä¢ Future-State Planning f√ºr neue Technologien und sich entwickelnde Bedrohungslandschaften\n\nüìä Architecture Design Principles:\n‚Ä¢ Layered Collection Strategy mit Hot-, Warm- und Cold-Storage-Tiers f√ºr optimale Performance und Kosteneffizienz\n‚Ä¢ Scalable Infrastructure Design zur Bew√§ltigung wachsender Datenvolumen ohne Performance-Degradation\n‚Ä¢ Redundancy und High Availability Planning f√ºr kritische Log-Streams und Business Continuity\n‚Ä¢ Geographic Distribution Considerations f√ºr globale Organisationen und Compliance-Anforderungen\n‚Ä¢ Integration-friendly Architecture f√ºr nahtlose Anbindung neuer Datenquellen und Tools\n\nüîÑ Data Flow Optimization:\n‚Ä¢ Intelligent Routing und Load Balancing f√ºr optimale Resource-Utilization und Processing-Effizienz\n‚Ä¢ Real-time vs. Batch Processing Decisions basierend auf Use Case Requirements und SLA-Vorgaben\n‚Ä¢ Data Compression und Deduplication Strategies zur Minimierung von Storage- und Bandwidth-Anforderungen\n‚Ä¢ Quality Gates und Validation Checkpoints zur Sicherstellung der Datenintegrit√§t entlang der Pipeline\n‚Ä¢ Monitoring und Alerting f√ºr Data Flow Health und Performance-Anomalien\n\n‚öñÔ∏è Compliance und Governance Integration:\n‚Ä¢ Regulatory Mapping zur Identifikation spezifischer Log-Anforderungen f√ºr verschiedene Compliance-Frameworks\n‚Ä¢ Data Classification und Sensitivity Labeling f√ºr angemessene Handling- und Retention-Policies\n‚Ä¢ Privacy-by-Design Implementation zur Minimierung von PII-Exposition und GDPR-Compliance\n‚Ä¢ Audit Trail Requirements Integration f√ºr vollst√§ndige Nachvollziehbarkeit aller Log-Operations\n‚Ä¢ Change Management Processes f√ºr kontrollierte Architektur-Anpassungen und Documentation\n\nüöÄ Performance und Scalability Engineering:\n‚Ä¢ Capacity Planning Models f√ºr predictive Scaling basierend auf Business Growth und Threat Evolution\n‚Ä¢ Resource Optimization Strategies f√ºr CPU-, Memory- und Storage-Effizienz\n‚Ä¢ Network Bandwidth Management f√ºr optimale Data Transfer ohne Business Impact\n‚Ä¢ Query Performance Optimization durch strategische Indexing und Data Partitioning\n‚Ä¢ Automated Scaling Mechanisms f√ºr dynamische Anpassung an schwankende Workloads"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 2),
        question: 'Welche Best Practices gelten f√ºr Log-Normalisierung und Parsing in SIEM-Umgebungen und wie gew√§hrleistet man Datenqualit√§t?',
        answer: "Log-Normalisierung und Parsing sind kritische Prozesse, die rohe Log-Daten in strukturierte, analysierbare Informationen transformieren. Effektive Normalisierung schafft die Grundlage f√ºr pr√§zise Korrelation, reduziert False Positives und erm√∂glicht konsistente Analytics across verschiedene Datenquellen.\n\nüîß Advanced Parsing Strategies:\n‚Ä¢ Schema-first Approach mit standardisierten Field Mappings f√ºr konsistente Datenstrukturen across alle Log-Quellen\n‚Ä¢ Multi-stage Parsing Pipeline mit spezialisierten Parsern f√ºr verschiedene Log-Formate und Komplexit√§tsstufen\n‚Ä¢ Regular Expression Optimization f√ºr Performance-kritische Parsing-Operations ohne Accuracy-Verlust\n‚Ä¢ Custom Parser Development f√ºr propriet√§re oder ungew√∂hnliche Log-Formate mit vollst√§ndiger Field-Extraktion\n‚Ä¢ Fallback Mechanisms f√ºr unbekannte oder malformed Log-Entries mit graceful Degradation\n\nüìã Data Normalization Framework:\n‚Ä¢ Common Information Model Implementation f√ºr einheitliche Field-Namen und Datentypen across alle Sources\n‚Ä¢ Taxonomy Standardization mit kontrollierten Vocabularies f√ºr Event-Kategorisierung und Threat-Classification\n‚Ä¢ Time Zone Normalization f√ºr accurate Temporal Correlation in Multi-Region Environments\n‚Ä¢ IP Address und Network Identifier Standardization f√ºr konsistente Network-based Analytics\n‚Ä¢ User Identity Normalization f√ºr unified User Behavior Analytics across verschiedene Systeme\n\nüéØ Quality Assurance Mechanisms:\n‚Ä¢ Real-time Validation Rules f√ºr immediate Detection von Parsing-Fehlern und Data Anomalien\n‚Ä¢ Statistical Quality Monitoring mit Baseline-Establishment f√ºr normale Parsing-Performance\n‚Ä¢ Field Completeness Tracking zur Identifikation von Missing Data und Parser-Ineffizienzen\n‚Ä¢ Data Type Consistency Checks f√ºr Enforcement von Schema-Compliance und Data Integrity\n‚Ä¢ Sampling-based Quality Assessment f√ºr Performance-optimierte Continuous Monitoring\n\nüîç Enrichment und Contextualization:\n‚Ä¢ Threat Intelligence Integration f√ºr automatische IOC-Tagging und Risk-Scoring von Events\n‚Ä¢ Asset Information Enrichment mit CMDB-Integration f√ºr Business Context und Criticality Assessment\n‚Ä¢ Geolocation Data Augmentation f√ºr Geographic-based Analytics und Anomaly Detection\n‚Ä¢ User Context Enhancement mit Identity Management System Integration f√ºr Behavioral Analytics\n‚Ä¢ Business Process Mapping f√ºr Application-aware Security Monitoring und Impact Assessment\n\n‚ö° Performance Optimization:\n‚Ä¢ Parallel Processing Architecture f√ºr High-Throughput Parsing ohne Latency-Penalties\n‚Ä¢ Memory-efficient Parsing Algorithms f√ºr Large-Scale Log Processing mit minimaler Resource-Utilization\n‚Ä¢ Caching Strategies f√ºr frequently accessed Enrichment Data und Lookup Tables\n‚Ä¢ Load Balancing und Auto-scaling f√ºr Dynamic Workload Distribution und Peak-Handling\n‚Ä¢ Monitoring und Alerting f√ºr Parser Performance und Resource Consumption Tracking\n\nüõ°Ô∏è Error Handling und Recovery:\n‚Ä¢ Comprehensive Error Classification mit spezifischen Recovery-Strategien f√ºr verschiedene Failure-Modes\n‚Ä¢ Dead Letter Queue Implementation f√ºr Failed Parsing Attempts mit Manual Review Capabilities\n‚Ä¢ Automatic Retry Mechanisms mit Exponential Backoff f√ºr Transient Failures\n‚Ä¢ Data Loss Prevention durch Redundant Processing Paths und Backup Mechanisms\n‚Ä¢ Audit Logging f√ºr alle Parsing-Operations und Error-Conditions f√ºr Troubleshooting und Compliance"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 3),
        question: 'Wie implementiert man effektive Real-time Log-Korrelation und welche Techniken optimieren die Erkennung komplexer Bedrohungsmuster?',
        answer: "Real-time Log-Korrelation ist das Herzst√ºck moderner SIEM-Systeme und erfordert sophisticated Algorithmen, die komplexe Bedrohungsmuster in Echtzeit erkennen k√∂nnen. Effektive Korrelation kombiniert regelbasierte Logik mit Machine Learning-Ans√§tzen f√ºr maximale Detection-Accuracy bei minimalen False Positives.\n\n‚ö° Real-time Processing Architecture:\n‚Ä¢ Stream Processing Framework Implementation f√ºr kontinuierliche Event-Analyse ohne Batch-Delays\n‚Ä¢ In-Memory Computing Strategies f√ºr Ultra-Low-Latency Correlation mit Sub-Second Response Times\n‚Ä¢ Distributed Processing Architecture f√ºr Horizontal Scaling und High-Availability Requirements\n‚Ä¢ Event Windowing Techniques f√ºr Time-based Correlation mit konfigurierbaren Zeitfenstern\n‚Ä¢ Priority Queue Management f√ºr Critical Event Processing und SLA-Compliance\n\nüß† Advanced Correlation Techniques:\n‚Ä¢ Multi-dimensional Correlation Rules mit komplexen Boolean Logic und Statistical Thresholds\n‚Ä¢ Temporal Pattern Recognition f√ºr Time-series Anomaly Detection und Attack Chain Reconstruction\n‚Ä¢ Behavioral Baseline Establishment mit Machine Learning f√ºr User und Entity Behavior Analytics\n‚Ä¢ Graph-based Correlation f√ºr Network Relationship Analysis und Lateral Movement Detection\n‚Ä¢ Fuzzy Logic Implementation f√ºr Probabilistic Threat Scoring und Risk Assessment\n\nüéØ Pattern Recognition Optimization:\n‚Ä¢ Signature-based Detection mit Regular Expression Optimization f√ºr Known Threat Patterns\n‚Ä¢ Anomaly Detection Algorithms f√ºr Unknown Threat Identification und Zero-Day Attack Recognition\n‚Ä¢ Statistical Analysis Integration f√ºr Deviation Detection und Trend Analysis\n‚Ä¢ Clustering Algorithms f√ºr Similar Event Grouping und Pattern Emergence Identification\n‚Ä¢ Neural Network Implementation f√ºr Complex Pattern Learning und Adaptive Threat Detection\n\nüìä Correlation Rule Management:\n‚Ä¢ Rule Lifecycle Management mit Version Control und Change Tracking f√ºr Audit Compliance\n‚Ä¢ Performance Monitoring f√ºr Rule Efficiency und Resource Consumption Optimization\n‚Ä¢ False Positive Reduction durch Continuous Rule Tuning und Threshold Adjustment\n‚Ä¢ Rule Prioritization und Execution Ordering f√ºr Optimal Processing Efficiency\n‚Ä¢ Automated Rule Generation basierend auf Threat Intelligence und Historical Attack Patterns\n\nüîÑ Context-aware Correlation:\n‚Ä¢ Asset Criticality Integration f√ºr Business-Impact-based Alert Prioritization\n‚Ä¢ User Role und Permission Context f√ºr Privilege-based Anomaly Detection\n‚Ä¢ Network Topology Awareness f√ºr Infrastructure-specific Threat Pattern Recognition\n‚Ä¢ Application Context Integration f√ºr Business-Process-aware Security Monitoring\n‚Ä¢ Threat Intelligence Enrichment f√ºr IOC-based Correlation und Attribution Analysis\n\nüöÄ Scalability und Performance:\n‚Ä¢ Horizontal Scaling Architecture f√ºr Growing Data Volumes und Correlation Complexity\n‚Ä¢ Resource Allocation Optimization f√ºr CPU-, Memory- und Storage-Efficient Processing\n‚Ä¢ Caching Strategies f√ºr Frequently Accessed Correlation Data und Lookup Tables\n‚Ä¢ Load Balancing f√ºr Even Distribution von Correlation Workloads across Processing Nodes\n‚Ä¢ Performance Metrics Tracking f√ºr Continuous Optimization und Capacity Planning"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 4),
        question: 'Welche Strategien gew√§hrleisten Compliance-konforme Log-Retention und wie optimiert man Audit-Readiness in SIEM-Umgebungen?',
        answer: "Compliance-konforme Log-Retention ist ein kritischer Aspekt des SIEM Log Managements, der rechtliche Anforderungen mit operativer Effizienz und Kostenoptimierung in Einklang bringen muss. Eine strategische Retention-Strategie gew√§hrleistet nicht nur Regulatory Compliance, sondern auch optimale Audit-Readiness und Forensic Capabilities.\n\nüìã Regulatory Compliance Framework:\n‚Ä¢ Comprehensive Compliance Mapping f√ºr alle relevanten Regulations wie GDPR, SOX, HIPAA, PCI-DSS und branchenspezifische Anforderungen\n‚Ä¢ Retention Period Matrix mit spezifischen Timeframes f√ºr verschiedene Log-Typen und Compliance-Kontexte\n‚Ä¢ Data Classification Schema f√ºr automatische Retention Policy Application basierend auf Content und Sensitivity\n‚Ä¢ Cross-Border Data Transfer Compliance f√ºr Multi-National Organizations und Cloud-Deployments\n‚Ä¢ Regular Compliance Assessment und Gap Analysis f√ºr Continuous Regulatory Alignment\n\nüóÑÔ∏è Intelligent Storage Tiering:\n‚Ä¢ Hot Storage f√ºr Recent High-Access Logs mit optimaler Query Performance und Real-time Analytics\n‚Ä¢ Warm Storage f√ºr Medium-Term Retention mit Balance zwischen Access Speed und Storage Costs\n‚Ä¢ Cold Storage f√ºr Long-Term Archival mit Cost-Optimized Solutions und Compliance-Focused Access\n‚Ä¢ Automated Data Lifecycle Management mit Policy-driven Migration zwischen Storage Tiers\n‚Ä¢ Compression und Deduplication Strategies f√ºr Storage Efficiency ohne Compliance Impact\n\n‚öñÔ∏è Legal Hold und eDiscovery:\n‚Ä¢ Legal Hold Management System f√ºr Preservation von Litigation-relevant Data √ºber normale Retention hinaus\n‚Ä¢ eDiscovery-Ready Data Formats mit standardisierten Export Capabilities und Chain of Custody\n‚Ä¢ Search und Retrieval Optimization f√ºr Legal Team Requirements und Court-Admissible Evidence\n‚Ä¢ Metadata Preservation f√ºr Complete Audit Trail und Forensic Analysis Capabilities\n‚Ä¢ Privacy Protection Mechanisms f√ºr PII Redaction w√§hrend Legal Proceedings\n\nüîç Audit Trail Optimization:\n‚Ä¢ Comprehensive Activity Logging f√ºr alle Log Management Operations und Administrative Actions\n‚Ä¢ Immutable Audit Records mit Cryptographic Integrity Protection und Tamper Detection\n‚Ä¢ Role-based Access Logging f√ºr Complete Visibility in User Activities und Permission Usage\n‚Ä¢ Change Management Documentation f√ºr All Configuration Modifications und Policy Updates\n‚Ä¢ Automated Audit Report Generation f√ºr Regular Compliance Reporting und Management Visibility\n\nüõ°Ô∏è Data Integrity und Security:\n‚Ä¢ Cryptographic Hash Verification f√ºr Log Integrity Assurance und Tampering Detection\n‚Ä¢ Encryption at Rest und in Transit f√ºr Complete Data Protection w√§hrend Retention Period\n‚Ä¢ Access Control Implementation mit Principle of Least Privilege und Need-to-Know Basis\n‚Ä¢ Backup und Disaster Recovery f√ºr Retention Data mit RTO/RPO Alignment zu Compliance Requirements\n‚Ä¢ Secure Deletion Procedures f√ºr End-of-Retention Data Disposal und Privacy Compliance\n\nüìä Cost Optimization Strategies:\n‚Ä¢ Storage Cost Analysis mit TCO Modeling f√ºr verschiedene Retention Scenarios und Technology Options\n‚Ä¢ Data Archival Automation f√ºr Reduced Operational Overhead und Consistent Policy Enforcement\n‚Ä¢ Cloud Storage Integration f√ºr Scalable und Cost-Effective Long-Term Retention Solutions\n‚Ä¢ Predictive Capacity Planning f√ºr Proactive Resource Allocation und Budget Management\n‚Ä¢ ROI Measurement f√ºr Retention Investment Justification und Continuous Improvement"
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQ batch 1 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
