import 'dotenv/config'
import { createClient } from '@sanity/client'

// Create client with direct credentials
const client = createClient({
  projectId: 'wwmm9rbb',
  dataset: 'production',
  apiVersion: '2024-02-14',
  token: process.env.SANITY_API_TOKEN,
  useCdn: false,
})

// Helper function to generate unique keys
function generateKey(prefix: string, index: number): string {
  return `${prefix}_${Date.now()}_${index}`
}

const run = async () => {
  try {
    console.log('Updating SIEM Implementierung page with FAQ batch 2...')
    
    // First, get the existing document
    console.log('Fetching existing document...')
    const existingDoc = await client.fetch('*[_id == $id][0]', { id: 'siem-implementierung' })
    
    if (!existingDoc) {
      throw new Error('Document "siem-implementierung" not found')
    }
    
    // Create new FAQs for SIEM Implementation technical deployment and configuration
    const newFaqs = [
      {
        _type: 'object',
        _key: generateKey('faq', 5),
        question: 'Wie gestaltet man eine optimale SIEM Infrastruktur-Architektur und welche Faktoren sind bei der technischen Dimensionierung entscheidend?',
        answer: "Die Infrastruktur-Architektur bildet das technische Fundament f√ºr erfolgreiche SIEM Implementierungen und entscheidet ma√ügeblich √ºber Performance, Skalierbarkeit und Verf√ºgbarkeit. Eine durchdachte Architektur ber√ºcksichtigt aktuelle Anforderungen und zuk√ºnftige Wachstumsszenarien gleicherma√üen.\n\nüèóÔ∏è Architektur-Design und Komponenten-Planung:\n‚Ä¢ Multi-Tier Architecture mit separaten Ebenen f√ºr Data Ingestion, Processing, Storage und Presentation\n‚Ä¢ Microservices-basierte Architektur f√ºr Flexibilit√§t und unabh√§ngige Skalierung einzelner Komponenten\n‚Ä¢ Load Balancer und Clustering-Strategien f√ºr optimale Performance-Verteilung und Ausfallsicherheit\n‚Ä¢ Separation of Concerns zwischen Hot Data, Warm Data und Cold Storage f√ºr kostenoptimierte Datenhaltung\n‚Ä¢ API Gateway und Service Mesh f√ºr sichere und performante Inter-Service Communication\n\nüìä Capacity Planning und Sizing-Strategien:\n‚Ä¢ Datenvolumen-Prognosen basierend auf aktuellen Log-Quellen und geplanten Erweiterungen\n‚Ä¢ Peak Load Analysis und Burst Capacity Planning f√ºr unvorhersehbare Datenspitzen\n‚Ä¢ Storage Growth Modeling mit verschiedenen Retention-Szenarien und Compliance-Anforderungen\n‚Ä¢ Compute Resource Planning f√ºr CPU-intensive Analytics und Real-time Processing\n‚Ä¢ Network Bandwidth Requirements f√ºr verschiedene Datenquellen und geografische Standorte\n\nüîÑ High Availability und Disaster Recovery:\n‚Ä¢ Active-Active oder Active-Passive Clustering f√ºr kontinuierliche Verf√ºgbarkeit\n‚Ä¢ Geographic Redundancy und Multi-Site Deployments f√ºr Disaster Recovery\n‚Ä¢ Automated Failover und Recovery-Mechanismen f√ºr minimale Downtime\n‚Ä¢ Data Replication Strategies f√ºr konsistente Datenverf√ºgbarkeit\n‚Ä¢ Backup und Restore-Prozeduren f√ºr verschiedene Recovery-Szenarien\n\n‚ö° Performance-Optimierung und Skalierung:\n‚Ä¢ Horizontal und Vertical Scaling-Strategien f√ºr verschiedene Workload-Typen\n‚Ä¢ Caching-Strategien und In-Memory Computing f√ºr beschleunigte Query-Performance\n‚Ä¢ Index-Optimierung und Data Partitioning f√ºr effiziente Datenabfragen\n‚Ä¢ Query Optimization und Resource Allocation f√ºr verschiedene Use Cases\n‚Ä¢ Real-time Monitoring und Auto-Scaling f√ºr dynamische Ressourcenanpassung\n\nüõ°Ô∏è Security-by-Design und Compliance:\n‚Ä¢ Network Segmentation und Micro-Segmentation f√ºr Defense-in-Depth\n‚Ä¢ Encryption at Rest und in Transit f√ºr umfassenden Datenschutz\n‚Ä¢ Identity und Access Management Integration f√ºr granulare Zugriffskontrolle\n‚Ä¢ Audit Logging und Compliance Monitoring der SIEM-Infrastruktur selbst\n‚Ä¢ Vulnerability Management und Security Hardening aller Komponenten\n\n‚òÅÔ∏è Cloud und Hybrid-Deployment Considerations:\n‚Ä¢ Cloud-native Services Integration f√ºr Skalierbarkeit und Kostenoptimierung\n‚Ä¢ Hybrid Cloud Strategies f√ºr Compliance und Data Sovereignty\n‚Ä¢ Container Orchestration und Kubernetes f√ºr moderne Deployment-Flexibilit√§t\n‚Ä¢ Infrastructure as Code f√ºr reproduzierbare und versionierte Deployments\n‚Ä¢ Cost Optimization Strategies f√ºr Cloud-basierte SIEM-Deployments"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 6),
        question: 'Welche Best Practices gelten f√ºr die SIEM Konfiguration und wie entwickelt man effektive Detection Rules und Correlation Logic?',
        answer: "Die SIEM Konfiguration ist der kritische Schritt, der aus einer technischen Plattform ein effektives Cybersecurity-Tool macht. Expertenkonfiguration und durchdachte Rule-Development sind entscheidend f√ºr optimale Threat Detection bei minimalen False Positives.\n\nüéØ Use Case-basierte Konfigurationsstrategie:\n‚Ä¢ Priorisierung kritischer Use Cases basierend auf Bedrohungslandschaft und Risikoprofil\n‚Ä¢ MITRE ATT&CK Framework Integration f√ºr comprehensive Threat Coverage\n‚Ä¢ Kill Chain-basierte Detection Logic f√ºr verschiedene Attack-Phasen\n‚Ä¢ Business-kritische Asset-fokussierte Monitoring-Strategien\n‚Ä¢ Compliance-driven Use Cases f√ºr regulatorische Anforderungen\n\nüîç Advanced Detection Rule Development:\n‚Ä¢ Behavioral Analytics und Machine Learning-basierte Anomaly Detection\n‚Ä¢ Signature-based Detection f√ºr bekannte Threat Patterns und IOCs\n‚Ä¢ Statistical Analysis und Threshold-basierte Alerting f√ºr quantitative Anomalien\n‚Ä¢ Context-aware Rules mit Enrichment aus externen Datenquellen\n‚Ä¢ Multi-stage Correlation f√ºr komplexe Attack-Szenarien und Campaign-Detection\n\n‚öôÔ∏è Correlation Engine Optimization:\n‚Ä¢ Event Correlation Windows und Time-based Aggregation f√ºr temporale Analyse\n‚Ä¢ Cross-source Correlation f√ºr umfassende Attack-Visibility\n‚Ä¢ Risk Scoring und Priority-basierte Alert-Klassifikation\n‚Ä¢ Deduplication und Event-Clustering f√ºr Alert-Fatigue Reduction\n‚Ä¢ Real-time und Batch-Processing Balance f√ºr verschiedene Detection-Anforderungen\n\nüìä Data Normalization und Parsing:\n‚Ä¢ Universal Event Format Definition f√ºr konsistente Datenstrukturen\n‚Ä¢ Custom Parser Development f√ºr propriet√§re Log-Formate\n‚Ä¢ Field Mapping und Taxonomy-Standardisierung f√ºr einheitliche Analytics\n‚Ä¢ Data Quality Validation und Cleansing f√ºr reliable Detection\n‚Ä¢ Timezone Normalization und Timestamp-Standardisierung f√ºr accurate Correlation\n\nüéõÔ∏è Alert Tuning und False Positive Management:\n‚Ä¢ Baseline-Establishment und Normal Behavior Profiling\n‚Ä¢ Iterative Tuning-Prozesse basierend auf Analyst-Feedback\n‚Ä¢ Whitelist und Exception-Management f√ºr bekannte False Positives\n‚Ä¢ Dynamic Threshold Adjustment basierend auf historischen Daten\n‚Ä¢ A/B Testing f√ºr Rule-Effectiveness und Performance-Impact\n\nüìà Performance und Scalability Considerations:\n‚Ä¢ Query Optimization und Index-Strategien f√ºr schnelle Rule-Execution\n‚Ä¢ Resource-intensive Rule Scheduling und Load-Balancing\n‚Ä¢ Memory und CPU-Usage Monitoring f√ºr Rule-Performance\n‚Ä¢ Parallel Processing und Distributed Computing f√ºr Large-Scale Analytics\n‚Ä¢ Rule Lifecycle Management und Deprecation-Strategien f√ºr System-Health"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 7),
        question: 'Wie implementiert man eine umfassende Datenquellen-Integration und gew√§hrleistet optimale Data Pipeline Performance?',
        answer: "Die Datenquellen-Integration ist das Herzst√ºck jeder SIEM Implementierung und bestimmt ma√ügeblich die Qualit√§t und Vollst√§ndigkeit der Security Analytics. Eine strategische Integration-Architektur gew√§hrleistet umfassende Visibility bei optimaler Performance.\n\nüîó Comprehensive Data Source Strategy:\n‚Ä¢ Asset Inventory und Data Source Mapping f√ºr vollst√§ndige Visibility-Abdeckung\n‚Ä¢ Critical Data Source Prioritization basierend auf Security-Value und Risk-Impact\n‚Ä¢ Legacy System Integration mit modernen API-Bridges und Protocol-Adaptern\n‚Ä¢ Cloud Service Integration f√ºr hybride und Multi-Cloud-Umgebungen\n‚Ä¢ Third-party Security Tool Integration f√ºr Enhanced Detection Capabilities\n\nüì° Data Collection und Ingestion Architecture:\n‚Ä¢ Agent-based und Agentless Collection-Strategien f√ºr verschiedene Umgebungen\n‚Ä¢ Real-time Streaming und Batch Processing f√ºr unterschiedliche Datentypen\n‚Ä¢ Message Queue und Buffer-Systeme f√ºr Resilient Data Ingestion\n‚Ä¢ Protocol Diversity Support von Syslog √ºber REST APIs bis hin zu propriet√§ren Formaten\n‚Ä¢ Data Compression und Optimization f√ºr Bandwidth-effiziente √úbertragung\n\n‚ö° Data Pipeline Optimization:\n‚Ä¢ Stream Processing und Event-driven Architecture f√ºr Low-Latency Analytics\n‚Ä¢ Data Partitioning und Sharding f√ºr Parallel Processing\n‚Ä¢ Caching-Strategien und In-Memory Processing f√ºr Performance-kritische Workflows\n‚Ä¢ Load Balancing und Auto-Scaling f√ºr dynamische Workload-Anpassung\n‚Ä¢ Circuit Breaker Patterns f√ºr Resilient Data Processing\n\nüîÑ Data Transformation und Enrichment:\n‚Ä¢ ETL Pipeline Design f√ºr strukturierte Datenaufbereitung\n‚Ä¢ Real-time Enrichment mit Threat Intelligence und Context-Daten\n‚Ä¢ Data Validation und Quality Assurance f√ºr reliable Analytics\n‚Ä¢ Schema Evolution und Backward Compatibility f√ºr sich √§ndernde Datenstrukte\n‚Ä¢ Custom Transformation Logic f√ºr spezifische Business-Anforderungen\n\nüìä Data Quality und Monitoring:\n‚Ä¢ Data Completeness Monitoring und Gap-Detection\n‚Ä¢ Latency Tracking und SLA-Monitoring f√ºr verschiedene Datenquellen\n‚Ä¢ Data Freshness Validation und Staleness-Alerting\n‚Ä¢ Volume Anomaly Detection f√ºr ungew√∂hnliche Datenmengen\n‚Ä¢ Error Rate Monitoring und Automatic Retry-Mechanismen\n\nüõ°Ô∏è Security und Compliance f√ºr Data Pipelines:\n‚Ä¢ End-to-End Encryption f√ºr sensitive Daten√ºbertragung\n‚Ä¢ Data Masking und Anonymization f√ºr Privacy-Compliance\n‚Ä¢ Access Control und Audit Logging f√ºr Data Pipeline Operations\n‚Ä¢ Data Lineage Tracking f√ºr Compliance und Forensic Analysis\n‚Ä¢ Retention Policy Enforcement und Automated Data Lifecycle Management"
      },
      {
        _type: 'object',
        _key: generateKey('faq', 8),
        question: 'Welche Testing-Strategien und Validierungsmethoden sind f√ºr SIEM Implementierungen kritisch und wie gew√§hrleistet man Operational Readiness?',
        answer: "Umfassendes Testing und systematische Validierung sind entscheidend f√ºr erfolgreiche SIEM Go-Lives und nachhaltige Operations. Eine strukturierte Testing-Strategie minimiert Risiken und gew√§hrleistet, dass das SIEM System die erwarteten Sicherheitsziele erf√ºllt.\n\nüß™ Multi-Level Testing Framework:\n‚Ä¢ Unit Testing f√ºr individuelle Rules und Detection Logic\n‚Ä¢ Integration Testing f√ºr End-to-End Data Flow und System-Interoperabilit√§t\n‚Ä¢ Performance Testing unter realistischen Load-Bedingungen und Stress-Szenarien\n‚Ä¢ Security Testing der SIEM-Implementation f√ºr Vulnerability Assessment\n‚Ä¢ User Acceptance Testing mit realen Szenarien und Stakeholder-Involvement\n\nüìä Data Quality und Detection Validation:\n‚Ä¢ Historical Data Replay f√ºr Rule-Effectiveness Testing\n‚Ä¢ Synthetic Attack Simulation f√ºr Detection Coverage Validation\n‚Ä¢ False Positive Rate Analysis und Baseline-Establishment\n‚Ä¢ Alert Response Time Measurement und SLA-Validation\n‚Ä¢ Detection Gap Analysis f√ºr Threat Coverage Assessment\n\n‚ö° Performance und Scalability Testing:\n‚Ä¢ Load Testing mit verschiedenen Datenvolumen und Ingestion-Raten\n‚Ä¢ Stress Testing f√ºr System-Limits und Breaking-Point Analysis\n‚Ä¢ Endurance Testing f√ºr Long-term Stability und Memory-Leak Detection\n‚Ä¢ Scalability Testing f√ºr Horizontal und Vertical Scaling-Scenarios\n‚Ä¢ Disaster Recovery Testing f√ºr Business Continuity Validation\n\nüîÑ Operational Readiness Assessment:\n‚Ä¢ Process Validation und Workflow-Testing f√ºr SOC Operations\n‚Ä¢ Runbook Testing und Incident Response Procedure Validation\n‚Ä¢ Team Readiness Assessment und Skill-Gap Analysis\n‚Ä¢ Tool Integration Testing f√ºr SOAR und Ticketing-System Connectivity\n‚Ä¢ Escalation Path Testing und Communication-Flow Validation\n\nüìã Go-Live Preparation und Cutover:\n‚Ä¢ Phased Rollout Strategy mit Pilot Groups und Gradual Expansion\n‚Ä¢ Parallel Running und Shadow Mode f√ºr Risk-free Transition\n‚Ä¢ Rollback Planning und Emergency Procedures f√ºr Contingency Scenarios\n‚Ä¢ Monitoring und Alerting Setup f√ºr Post-Go-Live Health Checks\n‚Ä¢ Success Criteria Definition und Go/No-Go Decision Framework\n\nüéØ Post-Implementation Validation:\n‚Ä¢ Business Value Measurement und ROI-Tracking\n‚Ä¢ User Satisfaction Surveys und Feedback Collection\n‚Ä¢ Performance Baseline Establishment f√ºr Continuous Improvement\n‚Ä¢ Lessons Learned Capture und Best Practice Documentation\n‚Ä¢ Continuous Monitoring und Health Check Automation f√ºr Sustained Operations"
      }
    ]
    
    // Update the document with new FAQs
    const updatedFaqs = [...(existingDoc.faq || []), ...newFaqs]
    
    console.log(`Adding ${newFaqs.length} new FAQs to the document...`)
    const transaction = client.transaction()
    transaction.patch(existingDoc._id, {
      set: {
        faq: updatedFaqs
      }
    })
    
    await transaction.commit()
    console.log('‚úÖ FAQ batch 2 added successfully')
  } catch (error) {
    console.error('Error:', error)
    throw error
  }
}

run()
