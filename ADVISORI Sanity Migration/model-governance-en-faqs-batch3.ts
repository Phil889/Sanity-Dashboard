import { createClient } from '@sanity/client'
import * as dotenv from 'dotenv'

dotenv.config()

const client = createClient({
  projectId: process.env.SANITY_PROJECT_ID || 'b0o7vqwz',
  dataset: 'production',
  useCdn: false,
  apiVersion: '2024-01-01',
  token: process.env.SANITY_API_TOKEN
})

// FAQs 9-12 for Model Governance EN
const faqsBatch3 = [
  {
    _key: `faq_${Date.now()}_9`,
    _type: 'object',
    question: 'What are best practices in Model Risk Management?',
    answer: `Model Risk Management (MRM) has established itself as an independent discipline to address the specific risks associated with the development and use of models. The following best practices have proven effective:

üèóÔ∏è Sound Framework
‚Ä¢ Risk-based tiering structure: Classification of models according to their risk potential and business criticality
‚Ä¢ Clear governance structure: Unambiguous assignment of responsibilities and decision-making authority
‚Ä¢ Three Lines of Defense: Separation of model development, independent validation, and audit
‚Ä¢ Comprehensive model risk policy: Documentation of binding principles and procedures
‚Ä¢ Control mechanisms: Implementation of effective controls in all phases of the model lifecycle

üìã Thorough Model Documentation
‚Ä¢ Complete specification: Detailed description of model purpose, methodology, and assumptions
‚Ä¢ Transparent data foundation: Documentation of all data sources, transformations, and quality controls
‚Ä¢ Traceable development steps: Justification of methodological decisions and rejected alternatives
‚Ä¢ Implementation details: Documentation of technical implementation and system integration
‚Ä¢ Usage guidelines: Clear description of permissible application scenarios and boundaries

üîç Robust Validation
‚Ä¢ Independent validation function: Organizational separation of development and validation
‚Ä¢ Multi-dimensional validation: Review of conceptual correctness, implementation, and performance
‚Ä¢ Rigorous testing procedures: Application of systematic testing approaches such as back-testing and stress testing
‚Ä¢ Challenger models: Development of alternative models for benchmarking and validation
‚Ä¢ Regular recertification: Periodic review of model suitability and performance

üìä Continuous Monitoring
‚Ä¢ Real-time monitoring: Ongoing control of model performance and data quality
‚Ä¢ Automated alerting mechanisms: Early warning of deviations and anomalies
‚Ä¢ Drift detection: Identification of data and concept drift affecting model performance
‚Ä¢ Performance tracking: Regular measurement and reporting of key model metrics
‚Ä¢ Outcome analysis: Comparison of model predictions with actual business outcomes

üîÑ Lifecycle Management
‚Ä¢ Structured development process: Defined stages from conception to deployment
‚Ä¢ Change management: Controlled introduction of model changes with appropriate review
‚Ä¢ Version control: Systematic tracking of model versions and their characteristics
‚Ä¢ Retirement planning: Proactive planning for model replacement or decommissioning
‚Ä¢ Knowledge transfer: Documentation and handover processes for model transitions`
  },
  {
    _key: `faq_${Date.now()}_10`,
    _type: 'object',
    question: 'How do you ensure model transparency and explainability?',
    answer: `Model transparency and explainability are central requirements for modern analytical and AI/ML models, especially in regulated industries and critical decision processes. They enable trust, traceability, and responsible model usage.

üîç Fundamentals of Model Transparency
‚Ä¢ Method transparency: Disclosure of algorithms and mathematical procedures used
‚Ä¢ Data transparency: Documentation of training data, their origin, quality, and limitations
‚Ä¢ Process transparency: Traceable description of the development and validation process
‚Ä¢ Usage transparency: Clarity about application scenarios and deployment boundaries of the model
‚Ä¢ Decision transparency: Disclosure of how model outputs flow into business decisions

‚öôÔ∏è Methods for Explainable AI (XAI)
‚Ä¢ Intrinsically interpretable models: Preference for inherently explainable algorithms such as decision trees, linear models, or rule-based systems
‚Ä¢ Post-hoc explainability methods: Application of techniques for subsequent explanation of complex models
‚Ä¢ Local explanations: Explanation of individual predictions through methods like LIME or SHAP
‚Ä¢ Global explanations: Overarching explanation of model behavior through Feature Importance, Partial Dependence Plots, or Global Surrogate Models
‚Ä¢ Counterfactual explanations: Showing what changes would lead to a different model result

üìä Visualization Techniques for Model Understanding
‚Ä¢ Feature importance plots: Visual representation of the influence of different features
‚Ä¢ Partial dependence plots: Visualization of the relationship between features and model results
‚Ä¢ SHAP value visualizations: Graphical representation of the contribution of individual features
‚Ä¢ Decision tree visualizations: Graphical representation of decision trees
‚Ä¢ Activation maps: Visualization of activations in neural networks (for image or text data)

üìã Documentation for Transparency
‚Ä¢ Model cards: Standardized documentation of model characteristics and limitations
‚Ä¢ Datasheets for datasets: Comprehensive documentation of training data
‚Ä¢ Explanation templates: Standardized formats for explaining model decisions
‚Ä¢ Audit trails: Complete logging of model development and deployment decisions
‚Ä¢ User documentation: Clear guidance for model users on interpretation and limitations

üéØ Stakeholder-Specific Explanations
‚Ä¢ Technical explanations: Detailed methodological explanations for data scientists
‚Ä¢ Business explanations: Impact-focused explanations for business stakeholders
‚Ä¢ Regulatory explanations: Compliance-oriented documentation for regulators
‚Ä¢ End-user explanations: Simple, actionable explanations for model consumers
‚Ä¢ Executive summaries: High-level overviews for senior management`
  },
  {
    _key: `faq_${Date.now()}_11`,
    _type: 'object',
    question: 'How do you validate and test AI/ML models?',
    answer: `Validation and testing of AI/ML models requires a comprehensive, multi-dimensional approach that goes beyond traditional testing procedures. A structured framework for model validation includes the following key elements:

üîç Conceptual Validation
‚Ä¢ Theoretical foundation: Review of the scientific and mathematical foundations of the model
‚Ä¢ Assumption validation: Assessment of the appropriateness and validity of all model assumptions
‚Ä¢ Method adequacy: Evaluation of the suitability of chosen algorithms for the use case
‚Ä¢ Conceptual limitations: Identification of conceptual boundaries and constraints
‚Ä¢ Alternative approaches: Comparison with other methodological approaches

üìä Input Validation and Data Quality
‚Ä¢ Data quality metrics: Systematic assessment of completeness, correctness, timeliness, etc.
‚Ä¢ Data coverage: Verification of the representativeness of training data for the target domain
‚Ä¢ Distribution analysis: Examination of distribution properties and changes
‚Ä¢ Bias detection: Identification of unwanted biases in training data
‚Ä¢ Data lineage: Traceability of data origin and transformations

‚öôÔ∏è Implementation Validation
‚Ä¢ Code review: Systematic review of implementation for errors and vulnerabilities
‚Ä¢ Unit tests: Isolated tests of individual model components and functions
‚Ä¢ Integration tests: Verification of correct collaboration of all model components
‚Ä¢ Reproducibility: Verification of consistency of results upon repeated execution
‚Ä¢ Performance tests: Review of efficiency and scalability of implementation

üìà Output Validation and Performance Measurement
‚Ä¢ Statistical metrics: Application of use-case-specific performance indicators (Accuracy, Precision, Recall, etc.)
‚Ä¢ Cross-validation: Use of k-fold cross-validation for robust performance assessment
‚Ä¢ Hold-out validation: Verification with separate test datasets
‚Ä¢ Temporal validation: Testing on data from different time periods
‚Ä¢ Segment analysis: Performance evaluation across different data segments

üß™ Specialized Testing Approaches
‚Ä¢ Stress testing: Assessment of model behavior under extreme conditions
‚Ä¢ Sensitivity analysis: Testing of model robustness to input variations
‚Ä¢ Adversarial testing: Evaluation of model resilience to adversarial inputs
‚Ä¢ Fairness testing: Assessment of model behavior across protected groups
‚Ä¢ Edge case testing: Verification of model behavior at boundary conditions

üîÑ Ongoing Validation
‚Ä¢ Backtesting: Regular comparison of predictions with actual outcomes
‚Ä¢ Champion-challenger testing: Comparison of production model with alternatives
‚Ä¢ A/B testing: Controlled experiments in production environment
‚Ä¢ Shadow mode testing: Parallel running of new models without affecting decisions
‚Ä¢ Continuous monitoring: Real-time tracking of model performance metrics`
  },
  {
    _key: `faq_${Date.now()}_12`,
    _type: 'object',
    question: 'What regulatory requirements exist for Model Governance?',
    answer: `Regulatory requirements for Model Governance have increased significantly in recent years, especially for the use of AI/ML models in critical application areas. These requirements vary by industry and region, with some central regulatory approaches emerging:

üè¶ Financial Sector-Specific Regulation
‚Ä¢ SR 11-7 (USA): The Federal Reserve guideline on model risk management as a fundamental standard
  - Comprehensive definition of model risk and its components
  - Requirements for robustly documented development processes
  - Necessity of independent validation and effective governance
  - Regular monitoring and continuous improvement
‚Ä¢ TRIM Guide (EU): Targeted Review of Internal Models by the European Central Bank
  - Harmonized assessment of internal models of banks
  - Detailed requirements for model validation and documentation
  - Focus on consistent and risk-appropriate model application
‚Ä¢ MaRisk (Germany): Minimum Requirements for Risk Management with specific provisions for model validation
‚Ä¢ PRA SS3/18 (UK): Supervisory Statement on model risk management in the banking sector
‚Ä¢ OSFI E-23 (Canada): Guidelines on Enterprise-wide Model Risk Management

üá™üá∫ EU AI Act and Related Regulations
‚Ä¢ Risk-based approach: Categorization of AI systems into different risk classes
‚Ä¢ Prohibited AI applications: Prohibition of AI systems with unacceptable risks
‚Ä¢ Requirements for high-risk AI:
  - Robust risk management systems
  - Data quality controls and governance
  - Technical documentation and audit trails
  - Human oversight and transparency
  - Accuracy, robustness, and cybersecurity
‚Ä¢ Transparency obligations: Information duties towards users of AI systems
‚Ä¢ Conformity assessment: Procedures for verifying compliance with requirements

üîí Data Protection Regulation Related to Models
‚Ä¢ GDPR/DSGVO: Requirements for automated decision-making
  - Right to explanation of automated decisions
  - Right to human intervention
  - Data minimization and purpose limitation
  - Privacy by design requirements
‚Ä¢ CCPA (California): Consumer rights regarding automated profiling
‚Ä¢ Sector-specific data protection: HIPAA (healthcare), GLBA (financial services)

üìã Industry-Specific Standards
‚Ä¢ Basel III/IV: Capital requirements with model-based calculations
‚Ä¢ Solvency II: Insurance regulation with internal model requirements
‚Ä¢ MDR/IVDR: Medical device regulations for AI in healthcare
‚Ä¢ FDA guidance: Requirements for AI/ML in medical devices
‚Ä¢ IOSCO principles: Securities regulation for algorithmic trading`
  }
]

async function addFaqsBatch3() {
  console.log('Adding FAQs batch 3 to Model Governance EN...')
  
  try {
    const result = await client
      .patch('model-governance-en')
      .setIfMissing({ faq: [] })
      .append('faq', faqsBatch3)
      .commit()
    
    console.log('FAQs batch 3 added successfully')
    console.log('Total FAQs now:', result.faq?.length || 0)
    return result
  } catch (error) {
    console.error('Error adding FAQs batch 3:', error)
    throw error
  }
}

addFaqsBatch3().catch(console.error)
