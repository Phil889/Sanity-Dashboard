import { createClient } from '@sanity/client'
import * as dotenv from 'dotenv'

dotenv.config()

const client = createClient({
  projectId: process.env.SANITY_PROJECT_ID || 'b0o7vqwz',
  dataset: 'production',
  useCdn: false,
  apiVersion: '2024-01-01',
  token: process.env.SANITY_API_TOKEN
})

// FAQs 13-16 for Model Governance EN
const faqsBatch4 = [
  {
    _key: `faq_${Date.now()}_13`,
    _type: 'object',
    question: 'How do you monitor models in production?',
    answer: `Effective monitoring of models in production is crucial for long-term model quality and risk minimization. A comprehensive monitoring framework encompasses several dimensions:

üìä Statistical Performance Monitoring
‚Ä¢ Model accuracy metrics: Continuous measurement of Accuracy, Precision, Recall, F1-Score, etc.
‚Ä¢ Population stability: Monitoring of target variable distribution stability over time
‚Ä¢ Discrimination capability: Control of model discriminatory power (e.g., AUC, Gini)
‚Ä¢ Calibration: Verification of agreement between predicted and actual probabilities
‚Ä¢ Confidence intervals: Calculation and monitoring of uncertainty measures for model predictions

üîç Drift Monitoring
‚Ä¢ Input drift: Detection of changes in input data distributions
‚Ä¢ Concept drift: Identification of changes in the relationship between input and output variables
‚Ä¢ Feature importance drift: Monitoring of shifts in relative influence of features
‚Ä¢ Segment-specific drift: Analysis of drift phenomena in specific customer segments
‚Ä¢ Threshold-based alerts: Automatic warnings when defined drift thresholds are exceeded

‚öôÔ∏è Operational Monitoring
‚Ä¢ Runtime performance: Monitoring of response times, throughput, and resource utilization
‚Ä¢ Availability: Control of model availability and downtime
‚Ä¢ Error detection: Identification and tracking of runtime errors and exceptions
‚Ä¢ API usage patterns: Analysis of request frequency, patterns, and volume
‚Ä¢ Infrastructure monitoring: Monitoring of underlying infrastructure and system resources

üîÑ Business-Oriented Monitoring
‚Ä¢ Business value: Measurement of actual business value and ROI of the model
‚Ä¢ Usage analysis: Monitoring of how and by whom the model is used
‚Ä¢ Outcome analysis: Comparison of model predictions with actual business results
‚Ä¢ Decision tracking: Tracking of decisions made based on model outputs
‚Ä¢ Customer impact: Assessment of model impact on customer experience and satisfaction

üìà Alerting and Response
‚Ä¢ Tiered alerting: Different alert levels based on severity and urgency
‚Ä¢ Escalation procedures: Clear paths for escalating critical issues
‚Ä¢ Automated responses: Automatic actions for certain types of alerts
‚Ä¢ On-call procedures: Defined responsibilities for responding to alerts
‚Ä¢ Incident management: Structured process for handling model incidents

üìã Reporting and Documentation
‚Ä¢ Regular performance reports: Scheduled reporting on model performance
‚Ä¢ Trend analysis: Identification of performance trends over time
‚Ä¢ Stakeholder dashboards: Customized views for different audiences
‚Ä¢ Audit trails: Complete logging of monitoring activities and findings
‚Ä¢ Regulatory reporting: Compliance with regulatory reporting requirements`
  },
  {
    _key: `faq_${Date.now()}_14`,
    _type: 'object',
    question: 'How do you handle Model Drift and model degradation?',
    answer: `Model Drift and model degradation are inevitable challenges in the lifecycle of AI/ML models. Effective handling of these phenomena requires a systematic approach to detection, analysis, and countermeasures:

üîç Detection of Drift and Degradation
‚Ä¢ Statistical drift detection: Use of distribution tests (KS test, PSI, JS divergence) to compare training and production data
‚Ä¢ Performance monitoring: Continuous monitoring of model performance metrics (Accuracy, F1-Score, etc.)
‚Ä¢ Concept drift detection: Detection of changes in the relationship between input and output
‚Ä¢ Segment analysis: Identification of drift in specific data segments or user groups
‚Ä¢ Early warning system: Implementation of thresholds and alerting mechanisms for early drift detection

üìä Classification and Analysis of Causes
‚Ä¢ Data drift: Changes in the distribution of input data without change in underlying relationships
‚Ä¢ Concept drift: Changes in the fundamental relationships between input and output variables
‚Ä¢ Gradual vs. abrupt drift: Distinction between slow changes and sudden shifts
‚Ä¢ Cyclical drift: Detection of seasonal or periodic patterns in model degradation
‚Ä¢ Root cause analysis: Systematic investigation of possible reasons for observed drift
  - External factors: Market changes, regulatory adjustments, consumer behavior
  - Internal factors: Changes in business processes, data collection, or processing
  - Technical factors: Changes in IT infrastructure or data sources

‚öôÔ∏è Strategies for Drift Management
‚Ä¢ Adaptive models: Implementation of online learning or regular incremental training
‚Ä¢ Ensemble methods: Combination of multiple models to increase robustness against drift
‚Ä¢ Windowing techniques: Training with sliding time windows of recent data
‚Ä¢ Weighting approaches: Higher weighting of recent data in model training
‚Ä¢ Trigger-based retraining: Automatic retraining when drift thresholds are exceeded

üîÑ Retraining and Model Updates
‚Ä¢ Scheduled retraining: Regular model updates on defined schedules
‚Ä¢ Event-driven retraining: Updates triggered by specific events or drift detection
‚Ä¢ Incremental learning: Continuous model updates with new data
‚Ä¢ Full retraining: Complete model rebuild when necessary
‚Ä¢ A/B testing: Controlled rollout of updated models

üìã Governance of Model Updates
‚Ä¢ Change management: Controlled process for model changes
‚Ä¢ Validation requirements: Re-validation of updated models
‚Ä¢ Documentation: Recording of all changes and their justification
‚Ä¢ Rollback procedures: Ability to revert to previous model versions
‚Ä¢ Stakeholder communication: Informing relevant parties of model changes`
  },
  {
    _key: `faq_${Date.now()}_15`,
    _type: 'object',
    question: 'How do you conduct Model Audits and Reviews?',
    answer: `Model audits and reviews are crucial mechanisms for quality assurance, risk minimization, and compliance assurance within the Model Governance framework. A systematic approach includes the following elements:

üìã Types of Model Reviews
‚Ä¢ Initial validation: Thorough review of new models before production deployment
‚Ä¢ Regular reviews: Periodic review at defined time intervals
‚Ä¢ Trigger-based reviews: Unscheduled reviews upon significant events
  - Performance degradation: Review when defined performance thresholds are breached
  - Significant changes: Review after substantial model or data changes
  - External factors: Review after relevant market or regulatory changes
‚Ä¢ Compliance audits: Specific review of compliance with regulatory requirements
‚Ä¢ Thematic reviews: Focused review of specific aspects (e.g., fairness, security)

üîç Key Components of a Model Audit
‚Ä¢ Methodological assessment: Review of conceptual correctness and method suitability
‚Ä¢ Implementation validation: Verification of correct technical implementation
‚Ä¢ Data quality review: Assessment of data used and data preparation processes
‚Ä¢ Performance evaluation: Analysis of model performance based on relevant metrics
‚Ä¢ Governance review: Verification of compliance with internal policies and processes
‚Ä¢ Documentation review: Assessment of completeness and quality of model documentation
‚Ä¢ Risk assessment: Identification and evaluation of model-specific risks
‚Ä¢ Compliance check: Verification of compliance with regulatory requirements

üë• Roles and Responsibilities
‚Ä¢ Independent reviewers: Ensuring organizational separation between development and audit
‚Ä¢ Subject matter experts: Involvement of domain experts for assessing technical appropriateness
‚Ä¢ Technical specialists: Review of technical aspects and implementation details
‚Ä¢ Model Risk Officers: Oversight of audit process and findings
‚Ä¢ Internal Audit: Independent assurance of governance effectiveness
‚Ä¢ External auditors: Third-party review for regulatory or assurance purposes

üìä Audit Process and Methodology
‚Ä¢ Planning: Definition of audit scope, objectives, and timeline
‚Ä¢ Information gathering: Collection of relevant documentation and data
‚Ä¢ Testing: Execution of audit procedures and tests
‚Ä¢ Analysis: Evaluation of findings against criteria and standards
‚Ä¢ Reporting: Documentation of findings, conclusions, and recommendations
‚Ä¢ Follow-up: Tracking of remediation actions and closure of findings

üìù Documentation and Reporting
‚Ä¢ Audit reports: Comprehensive documentation of audit findings
‚Ä¢ Finding classification: Categorization of issues by severity and risk
‚Ä¢ Remediation tracking: Monitoring of corrective actions
‚Ä¢ Management reporting: Summary reports for senior management
‚Ä¢ Regulatory reporting: Documentation for regulatory examinations
‚Ä¢ Lessons learned: Capture of insights for process improvement`
  },
  {
    _key: `faq_${Date.now()}_16`,
    _type: 'object',
    question: 'What KPIs should be monitored for Model Governance?',
    answer: `Effective Model Governance requires systematic monitoring of specific Key Performance Indicators (KPIs) that make the quality, risks, and value contribution of models measurable. A comprehensive KPI framework for Model Governance encompasses various dimensions:

üìä Model Quality and Performance KPIs
‚Ä¢ Statistical performance metrics: Accuracy, Precision, Recall, F1-Score, AUC, RMSE, etc.
‚Ä¢ Model stability: Population Stability Index (PSI), Characteristic Stability Index (CSI)
‚Ä¢ Calibration: Brier Score, Expected Calibration Error (ECE)
‚Ä¢ Discrimination capability: Gini coefficient, Kolmogorov-Smirnov statistic
‚Ä¢ Robustness: Performance variance across different data segments and time periods
‚Ä¢ Comparison metrics: Performance relative to benchmark or predecessor models
‚Ä¢ Degradation rate: Speed of performance decline over time

üîç Risk and Compliance KPIs
‚Ä¢ Model risk score: Aggregated assessment of overall risk of a model
‚Ä¢ Validation quality: Scope and depth of validations performed
‚Ä¢ Compliance rate: Degree of compliance with relevant regulatory requirements
‚Ä¢ Documentation quality: Completeness and timeliness of model documentation
‚Ä¢ Override rate: Frequency of manual overrides of model decisions
‚Ä¢ Incident rate: Number of model-related incidents and problems
‚Ä¢ Time-to-resolution: Duration until resolution of identified model problems

‚öñÔ∏è Fairness and Ethics KPIs
‚Ä¢ Demographic parity: Equality of outcome distribution across different groups
‚Ä¢ Equal opportunity: Equality of True Positive Rate across different groups
‚Ä¢ Disparate impact: Ratio of positive outcomes between different groups
‚Ä¢ Group fairness metrics: Statistical Parity, Equalized Odds, etc.
‚Ä¢ Explainability score: Degree of interpretability and explainability of the model
‚Ä¢ Bias metrics: Quantification of unwanted biases in model predictions
‚Ä¢ Fairness monitoring: Tracking of fairness metrics over time

‚öôÔ∏è Operational KPIs
‚Ä¢ Model availability: Uptime and availability of models in production
‚Ä¢ Response time: Latency of model predictions
‚Ä¢ Throughput: Number of predictions processed per time unit
‚Ä¢ Resource utilization: CPU, memory, and storage usage
‚Ä¢ Error rate: Frequency of technical errors and exceptions
‚Ä¢ Deployment frequency: Rate of model updates and deployments
‚Ä¢ Rollback rate: Frequency of model rollbacks due to issues

üíº Business Value KPIs
‚Ä¢ ROI: Return on investment for model development and operation
‚Ä¢ Business impact: Measurable business outcomes attributed to models
‚Ä¢ Decision quality: Improvement in decision quality through model usage
‚Ä¢ Cost savings: Reduction in costs through model automation
‚Ä¢ Revenue impact: Revenue contribution from model-driven decisions
‚Ä¢ Customer satisfaction: Impact on customer experience metrics
‚Ä¢ Time-to-value: Time from model development to business value realization

üìà Governance Process KPIs
‚Ä¢ Validation cycle time: Duration of validation processes
‚Ä¢ Approval turnaround: Time from submission to approval
‚Ä¢ Documentation completeness: Percentage of models with complete documentation
‚Ä¢ Training coverage: Percentage of staff trained on governance requirements
‚Ä¢ Audit findings: Number and severity of audit findings
‚Ä¢ Remediation rate: Speed of addressing identified issues`
  }
]

async function addFaqsBatch4() {
  console.log('Adding FAQs batch 4 to Model Governance EN...')
  
  try {
    const result = await client
      .patch('model-governance-en')
      .setIfMissing({ faq: [] })
      .append('faq', faqsBatch4)
      .commit()
    
    console.log('FAQs batch 4 added successfully')
    console.log('Total FAQs now:', result.faq?.length || 0)
    return result
  } catch (error) {
    console.error('Error adding FAQs batch 4:', error)
    throw error
  }
}

addFaqsBatch4().catch(console.error)
